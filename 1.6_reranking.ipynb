{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Large Corpus RAG with Re-ranking\n",
    "\n",
    "Same BEIR/NQ setup as 1.4, but with a two-stage retrieval pipeline:\n",
    "\n",
    "1. **Retrieve** a broad set of candidates (top-50) using fast embedding similarity\n",
    "2. **Re-rank** those candidates using an LLM that reads each document and scores its relevance\n",
    "3. **Keep** only the top-5 after re-ranking\n",
    "\n",
    "Embeddings are fast but shallow -- they match on surface-level similarity. A re-ranker reads the actual text and can catch semantic relevance that embeddings miss. The tradeoff is cost: you're making one LLM call per candidate document per query.\n",
    "\n",
    "The key insight: retrieve broadly, then filter precisely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenRouter client with the OpenAI SDK\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "RERANK_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "# Retrieval settings\n",
    "INITIAL_K = 50    # broad retrieval\n",
    "RERANK_K = 5      # keep after re-ranking\n",
    "TOP_K = RERANK_K  # for Recall@K reporting\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BEIR/NQ queries and relevance judgments...\n",
      "Selected 50 queries, 61 gold documents\n",
      "Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\n",
      "Locating gold documents in corpus...\n",
      "Found 61/61 gold documents\n",
      "Corpus: 10000 docs (61 gold + 9939 distractors)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit! Reusing collection 'nq_10000_b1bf36b34ec3' (10,000 docs, no re-embedding needed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(\n",
    "    question: str,\n",
    "    doc_ids: list[str],\n",
    "    doc_texts: list[str],\n",
    "    top_k: int,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Re-rank retrieved documents using an LLM.\n",
    "\n",
    "    Asks the model to score each document's relevance to the question\n",
    "    on a 0-10 scale, then returns the top_k highest-scored documents.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question\n",
    "        doc_ids: List of document IDs from initial retrieval\n",
    "        doc_texts: List of document texts from initial retrieval\n",
    "        top_k: Number of documents to keep after re-ranking\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (reranked_ids, reranked_texts) with top_k items\n",
    "    \"\"\"\n",
    "    # Build a numbered list of documents for the LLM\n",
    "    doc_list = \"\\n\\n\".join(\n",
    "        f\"[Doc {i+1}] {text[:500]}\" for i, text in enumerate(doc_texts)\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=RERANK_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are a relevance scorer. Given a question and a list of documents, \"\n",
    "                \"score each document's relevance to the question on a 0-10 scale.\\n\"\n",
    "                \"Respond with ONLY a JSON array of objects, one per document:\\n\"\n",
    "                '[{\"doc\": 1, \"score\": 8}, {\"doc\": 2, \"score\": 2}, ...]\\n'\n",
    "                \"Score 10 = directly answers the question, 0 = completely irrelevant.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nDocuments:\\n{doc_list}\"},\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "    # Parse scores and re-rank\n",
    "    try:\n",
    "        content = response.choices[0].message.content or \"[]\"\n",
    "        # Extract JSON array from response (handle markdown code blocks)\n",
    "        if \"```\" in content:\n",
    "            content = content.split(\"```\")[1]\n",
    "            if content.startswith(\"json\"):\n",
    "                content = content[4:]\n",
    "        scores = json.loads(content.strip())\n",
    "        # Build (score, index) pairs and sort descending\n",
    "        scored = []\n",
    "        for item in scores:\n",
    "            idx = item.get(\"doc\", 0) - 1  # 1-indexed to 0-indexed\n",
    "            if 0 <= idx < len(doc_texts):\n",
    "                scored.append((item.get(\"score\", 0), idx))\n",
    "        scored.sort(reverse=True)\n",
    "        top_indices = [idx for _, idx in scored[:top_k]]\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        # Fallback: keep original order\n",
    "        top_indices = list(range(min(top_k, len(doc_texts))))\n",
    "\n",
    "    return (\n",
    "        [doc_ids[i] for i in top_indices],\n",
    "        [doc_texts[i] for i in top_indices],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the eval model with retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        retrieved_docs: List of document texts retrieved from the collection\n",
    "\n",
    "    Returns:\n",
    "        The generated answer as a string\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\n",
    "Your task is to:\n",
    "1. Carefully read all provided documents\n",
    "2. Find the information needed to answer the question\n",
    "3. Provide a clear, concise answer based ONLY on the documents\n",
    "\n",
    "If the answer cannot be found in the documents, say so explicitly.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question based on the provided documents.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=EVAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "def score_answer(\n",
    "    question: str,\n",
    "    documents: list[str],\n",
    "    generated_answer: str,\n",
    ") -> tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Score the generated answer using the scoring model.\n",
    "\n",
    "    Args:\n",
    "        question: The original question\n",
    "        documents: The retrieved documents used to generate the answer\n",
    "        generated_answer: The answer generated by the eval model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (score out of 100, explanation)\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\n",
    "Evaluate the answer on these criteria:\n",
    "1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n",
    "2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n",
    "3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n",
    "4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "{\n",
    "    \"score\": <integer from 0-100>,\n",
    "    \"reasoning\": \"<brief explanation of the score>\"\n",
    "}\"\"\"\n",
    "\n",
    "    eval_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=SCORING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score_data = json.loads(response.choices[0].message.content or \"{}\")\n",
    "        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return 0, \"Error parsing score response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate with two-stage retrieval: broad retrieve then LLM re-rank.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    # Stage 1: broad retrieval\n",
    "    results = collection.query(query_texts=[question], n_results=INITIAL_K)\n",
    "    initial_ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "    initial_docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "    # Stage 2: LLM re-ranking\n",
    "    reranked_ids, reranked_docs = rerank_documents(\n",
    "        question, initial_ids, initial_docs, top_k\n",
    "    )\n",
    "\n",
    "    # Recall@K after re-ranking\n",
    "    hits = len(gold_ids & set(reranked_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    # Also compute recall before re-ranking for comparison\n",
    "    initial_hits = len(gold_ids & set(initial_ids[:top_k]))\n",
    "    initial_recall = initial_hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    generated_answer = generate_answer(question, reranked_docs)\n",
    "    score, reasoning = score_answer(question, reranked_docs, generated_answer)\n",
    "\n",
    "    return example_index, {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"recall_before_rerank\": initial_recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries: list[dict],\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    "    max_workers: int = 8,\n",
    ") -> dict:\n",
    "    \"\"\"Run evaluation on all queries using parallel workers.\"\"\"\n",
    "    eval_size = len(eval_queries)\n",
    "    print(f\"Running evaluation on {eval_size} queries with \"\n",
    "          f\"{max_workers} parallel workers...\\n\")\n",
    "\n",
    "    results_by_index: dict[int, dict] = {}\n",
    "    scores, recalls, recalls_before = [], [], []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(\n",
    "                evaluate_single_example,\n",
    "                eval_queries[i], i, collection, qrels, top_k\n",
    "            ): i for i in range(eval_size)\n",
    "        }\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                recalls_before.append(result[\"recall_before_rerank\"])\n",
    "                completed += 1\n",
    "                print(f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question: {result['question'][:80]}...\")\n",
    "                print(\n",
    "                    f\"  Recall: {result['recall_before_rerank']:.2f} -> {result['recall_at_k']:.2f} (after rerank)\")\n",
    "                print(f\"  Score: {result['score']}/100\")\n",
    "                print(f\"  Reasoning: {result['scoring_reasoning']}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    ordered = [results_by_index[i]\n",
    "               for i in range(eval_size) if i in results_by_index]\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_recall_before = sum(recalls_before) / \\\n",
    "        len(recalls_before) if recalls_before else 0\n",
    "\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL,\n",
    "        \"scoring_model\": SCORING_MODEL,\n",
    "        \"rerank_model\": RERANK_MODEL,\n",
    "        \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(),\n",
    "        \"initial_k\": INITIAL_K,\n",
    "        \"top_k\": top_k,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(avg_score, 2),\n",
    "        \"avg_recall_at_k\": round(avg_recall, 4),\n",
    "        \"avg_recall_before_rerank\": round(avg_recall_before, 4),\n",
    "        \"individual_scores\": scores,\n",
    "        \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": ordered,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 50 queries with 8 parallel workers...\n",
      "\n",
      "[1/50] Example 8\n",
      "  Question: in order to prove disparate impact you first must establish...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and directly extracts the necessary information from Document 1. It correctly identifies the requirement for proving disparate impact as established in the provided text.\n",
      "\n",
      "[2/50] Example 6\n",
      "  Question: who were the three elves who got rings...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct and faithful to the provided documents. It correctly identifies the three individuals (Gil-galad, Círdan, and Galadriel) and explains the discrepancy between the two versions of the story mentioned in Document 1. The answer is clear and fully addresses the question using only the provided text.\n",
      "\n",
      "[3/50] Example 2\n",
      "  Question: how many episodes are in chicago fire season 4...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on the provided documents. Document 1 explicitly states that 'The season contained 23 episodes.' The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[4/50] Example 5\n",
      "  Question: nitty gritty dirt band fishin in the dark album...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate, faithful to the provided documents, and directly answers the question. It correctly identifies the album 'Hold On' and the release date as specified in Document 1.\n",
      "\n",
      "[5/50] Example 3\n",
      "  Question: who sings love will keep us alive by the eagles...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, complete, and faithful to the provided documents. It correctly identifies Timothy B. Schmit as the lead vocalist for the song 'Love Will Keep Us Alive' based on Document 1 and provides the context of the performance.\n",
      "\n",
      "[6/50] Example 10\n",
      "  Question: who makes the decisions about what to produce in a market economy...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies the core mechanism of decision-making in a market economy (interplay of supply and demand) as described in the documents. It correctly identifies the role of capital and financial markets and the interaction between buyers and sellers. It is faithful to the text, clear, and complete.\n",
      "\n",
      "[7/50] Example 9\n",
      "  Question: where do characters live in this is us...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. It correctly identifies the locations for all main characters mentioned in Document 1, distinguishing between their adult residences and their childhood home in Pittsburgh. The information is clear and well-organized.\n",
      "\n",
      "[8/50] Example 4\n",
      "  Question: who is the leader of the ontario pc party...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate based on the provided documents. It correctly identifies Patrick Brown as the leader of the Ontario PC Party, citing both Document 1 and Document 2. The response is clear, faithful to the text, and complete.\n",
      "\n",
      "[9/50] Example 14\n",
      "  Question: when did they stop cigarette advertising on television...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and directly addresses the question using the provided documents. It correctly identifies the date (January 2, 1971) and cites the specific legislative act mentioned in Document 1.\n",
      "\n",
      "[10/50] Example 15\n",
      "  Question: who has been chosen as the brand ambassador of the campaign 'beti bachao-beti pa...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate, faithful to the documents, and clearly written. Document 1 explicitly states that Sakshi Malik was made the brand ambassador for BBBP (Beti Bachao-Beti Padhao) on 26 August 2016.\n",
      "\n",
      "[11/50] Example 13\n",
      "  Question: who plays the doc in back to the future...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to the provided documents. It correctly identifies Christopher Lloyd as the actor who plays 'Doc' and provides the specific supporting evidence from the text.\n",
      "\n",
      "[12/50] Example 11\n",
      "  Question: when is the new tappan zee bridge going to be finished...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate based on Document 1. it correctly identifies the expected completion/operational date of June 15, 2018, and provides the necessary context regarding the progression of the north and south spans. It is clear, faithful to the text, and fully answers the question.\n",
      "\n",
      "[13/50] Example 1\n",
      "  Question: what is non controlling interest on balance sheet...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate, complete, and faithful to the provided documents. it correctly defines non-controlling interest using Document 2 and explains how it is reported on the balance sheet using Document 1. The tone is clear and well-organized.\n",
      "\n",
      "[14/50] Example 16\n",
      "  Question: how many seasons of prison break are on netflix...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and faithful to the documents. Document 1 states that the first five seasons have been released on DVD/Blu-ray and that the show is available on Netflix, but it does not explicitly state that all five seasons (or any specific number) are currently on the Netflix platform. The answer correctly identifies this limitation in the source text.\n",
      "\n",
      "[15/50] Example 21\n",
      "  Question: how many lines of symmetry are there in a equilateral triangle...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, complete, and faithful to the provided documents. Document 1 explicitly states that an equilateral triangle has 3 lines of reflection (symmetry).\n",
      "\n",
      "[16/50] Example 12\n",
      "  Question: who recorded i can't help falling in love with you...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[17/50] Example 20\n",
      "  Question: where are the mitochondria located in the sperm...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and directly supported by the documents. It correctly identifies the 'midpiece' as the location and provides specific details from both Document 1 and Document 2 to support the answer. It is clear, concise, and faithful only to the provided text.\n",
      "\n",
      "[18/50] Example 22\n",
      "  Question: how many seasons of the oc are there...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1, which explicitly refers to the 'fourth and final season' of the show. The answer is faithful to the text, clear, and complete.\n",
      "\n",
      "[19/50] Example 25\n",
      "  Question: who were farmers who kept a small portion of their crops & gave the rest to the ...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer identifies the correct term ('sharecroppers') based on Document 1. It accurately describes the relationship between the tenant and the landowner as defined in the provided text. The response is concise, faithful to the source material, and fully answers the question.\n",
      "\n",
      "[20/50] Example 18\n",
      "  Question: when did the us take over wake island...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. It correctly distinguishes between the initial flag-raising on July 4, 1898, and the formal act of possession on January 17, 1899, using only the provided documents. It is clear, complete, and faithful to the text.\n",
      "\n",
      "[21/50] Example 19\n",
      "  Question: when did american two party system began to emerge...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and well-supported by the provided documents. It correctly identifies the time period (1790s/Washington administration) and the specific historical context (the Whiskey Rebellion and the First Party System) mentioned in Document 5 and Document 1. It is faithful to the text and clearly explains the origins as described in the sources.\n",
      "\n",
      "[22/50] Example 27\n",
      "  Question: 2. what are the reasons states impose protectionists policies on other countries...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies and categorizes all reasons for protectionism mentioned in the provided documents, including economic shielding, mercantilism, intellectual property protection, and national security/political stability. It is faithful to the text and clearly organized with citations.\n",
      "\n",
      "[23/50] Example 28\n",
      "  Question: where are they building the new raiders stadium...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate, faithful to the provided documents, and addresses the question with specific details (location coordinates and city) mentioned in Document 1. It is clear and well-organized.\n",
      "\n",
      "[24/50] Example 23\n",
      "  Question: latest season on keeping up with the kardashians...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate based on Document 1, which states that 205 episodes have aired concluding fourteen seasons as of December 2017. It stays faithful to the provided text and clearly answers the question.\n",
      "\n",
      "[25/50] Example 26\n",
      "  Question: what is pumped up kicks the song about...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. It accurately synthesizes information from the provided documents, specifically Documents 1 and 2, to explain the meaning, perspective, and intent behind the song. It correctly identifies the contrast between the music and lyrics, the meaning of the title, and the songwriter's motivations. It remains faithful to the text without including outside information.\n",
      "\n",
      "[26/50] Example 7\n",
      "  Question: converting stereo signal to mono signal is called...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and faithful to the provided documents. Document 1 describes the process of panning both channels 'straight up' to create a 'dual mono signal,' and the generated answer correctly notes that while this process is described, a specific single-word term for the conversion is not explicitly defined in the text.\n",
      "\n",
      "[27/50] Example 24\n",
      "  Question: how dose the poet present death as a voyage in crossing the bar...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies the core metaphor of 'crossing the bar' as a voyage from Document 1, correctly interprets the figure of the Pilot as God based on the provided text, and maintains strict faithfulness to the source material. It is clearly organized and directly answers the question.\n",
      "\n",
      "[28/50] Example 33\n",
      "  Question: what are bulls used for on a farm...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies the uses of bulls (breeding, semen collection/AI, meat production, and draft work) based on the provided documents. It correctly notes that draft animals are usually castrated (oxen) and maintains strict faithfulness to the text without including outside information.\n",
      "\n",
      "[29/50] Example 30\n",
      "  Question: does joe die in the purge election year...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to Document 1. it correctly identifies that Joe dies after being fatally wounded by Harmon James and provides the specific detail regarding his final request to Marcos.\n",
      "\n",
      "[30/50] Example 36\n",
      "  Question: what is the meaning of the dragon boat festival...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate and faithful to the provided documents, specifically Document 1. It correctly identifies the alternative names, the significance (fealty and filial piety), and its timing according to the Chinese calendar. It does not include outside information or hallucinations.\n",
      "\n",
      "[31/50] Example 37\n",
      "  Question: who is edmund on days of our lives...\n",
      "  Recall: 0.00 -> 1.00 (after rerank)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[32/50] Example 17\n",
      "  Question: what was the name of atom bomb dropped by usa on hiroshima...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely correct, faithful to the provided documents, and directly answers the question with clarity. It correctly identifies the name of the bomb ('Little Boy') and its type ('uranium gun-type') as mentioned in Document 1.\n",
      "\n",
      "[33/50] Example 29\n",
      "  Question: what event provoked congress to propose the eleventh amendment and the states to...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. It correctly identifies Chisholm v. Georgia as the provocative event and explains the legal context and reasoning for the amendment as described in Documents 1 and 2. The response is clear and fully addresses the question.\n",
      "\n",
      "[34/50] Example 34\n",
      "  Question: is spain the second largest country in europe...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on Document 1. It correctly distinguishes between Spain's ranking in Western Europe (2nd) versus the European continent as a whole (4th), directly answering the question with supporting details from the text.\n",
      "\n",
      "[35/50] Example 40\n",
      "  Question: royal society for the protection of birds number of members...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate according to the provided documents. it provides the specific figure cited in Document 1, includes the breakdown of youth members, and does not include any information outside of the provided text.\n",
      "\n",
      "[36/50] Example 31\n",
      "  Question: what is the meaning of the name comanche...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate, complete, and faithful to the provided documents. It correctly identifies the origin and meaning of the name according to Document 1.\n",
      "\n",
      "[37/50] Example 32\n",
      "  Question: who do you meet at the gates of heaven...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct, complete, and faithful to Document 1, which specifically addresses the question about the gates of heaven. It correctly identifies Saint Peter and his role as described in the text without including outside information.\n",
      "\n",
      "[38/50] Example 43\n",
      "  Question: when did athens emerges as wealthiest greek city state...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and directly addresses the question using the provided documents. It correctly identifies the time period (late 6th century BCE) and includes the supporting detail regarding silver veins mentioned in Document 1. It is clear, faithful to the text, and complete.\n",
      "\n",
      "[39/50] Example 41\n",
      "  Question: when was theme from a summer place released...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and complete based on the provided documents. Document 1 explicitly states that the single was released in September 1959 on Columbia Records, prior to the film's release in November 1959. The answer is clear and faithful to the text.\n",
      "\n",
      "[40/50] Example 39\n",
      "  Question: what degree is a crock pot on low...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct and faithful to the documents. Document 1 provides temperatures for the 'warming' mode (71–74 °C / 160–165 °F) but does not state the specific temperature for a 'low' setting. The answer accurately identifies what is present and what is missing from the text.\n",
      "\n",
      "[41/50] Example 38\n",
      "  Question: i want to be with you everywhere song...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, complete, and faithful to the provided documents. It correctly identifies the song 'Everywhere' by Fleetwood Mac from Document 1, providing all the relevant details including the artist, writer, album, release dates, and chart performance. The information is clearly organized.\n",
      "\n",
      "[42/50] Example 46\n",
      "  Question: who invented the frisbee and how did it get its name...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct and faithful to the documents. It correctly identifies Fred Morrison as the inventor (document 1 mentions 'he' in the context of Morrison later in the paragraph) and accurately explains the naming process involving college students and the Wham-O company. The answer is clear and well-organized.\n",
      "\n",
      "[43/50] Example 47\n",
      "  Question: who plays v on orange is the new black...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and directly supported by Document 1. It correctly identifies the actress (Toussaint) and providing the full character name (Yvonne \"Vee\" Parker) as mentioned in the text.\n",
      "\n",
      "[44/50] Example 48\n",
      "  Question: how long prime minister stay in office canada...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and directly addresses the question using only the info provided in Document 1. It correctly identifies that there is no fixed term and lists the specific conditions for leaving office.\n",
      "\n",
      "[45/50] Example 50\n",
      "  Question: when do willow and tara get back together...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate according to Document 1. It correctly identifies the episode ('Seeing Red') where they reconcile and provides the necessary context from the text. It is clear, concise, and avoids information not found in the provided documents.\n",
      "\n",
      "[46/50] Example 42\n",
      "  Question: when does jenny humphrey come back to gossip girl...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. Document 1 explicitly states that Jenny returns in the final episode during a five-year time jump for her brother's wedding, and it includes the details about her career which the answer correctly summarizes.\n",
      "\n",
      "[47/50] Example 35\n",
      "  Question: difference between single layer perceptron and multilayer perceptron...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate based on Document 1. It correctly identifies the structural differences, the different activation functions used, and the functional capabilities (binary classification vs. classification/regression) as described in the text. The answer is clear, well-organized, and strictly follows the provided information.\n",
      "\n",
      "[48/50] Example 45\n",
      "  Question: who sings war don't let me down...\n",
      "  Recall: 1.00 -> 1.00 (after rerank)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate based on the provided documents. It correctly identifies the performers (The Chainsmokers featuring Daya) using information from Document 1. The answer is clear, faithful to the text, and complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, RERANK_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"Rerank model :    {results['rerank_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.6_reranking.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
