{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Large Corpus RAG Evaluation\n",
    "\n",
    "Previous scripts (1.1-1.3) used ~10 documents per question. That barely tests retrieval -- when the haystack has 10 items, even bad embeddings find the needle.\n",
    "\n",
    "This notebook uses the **BEIR Natural Questions** dataset: 2.68M Wikipedia passages and 3,452 Google Search questions with ground-truth relevance labels. We build ONE shared ChromaDB collection with thousands of documents and measure both **retrieval quality** (Recall@K) and **answer quality** (LLM-as-judge).\n",
    "\n",
    "Key differences from 1.2 (naive RAG):\n",
    "\n",
    "- ONE shared collection vs. per-question ephemeral collections\n",
    "- 10,000+ documents vs. ~10 documents\n",
    "- Ground-truth relevance labels to measure Recall@K\n",
    "- Persistent ChromaDB so we don't re-embed on every run\n",
    "- BEIR/NQ dataset instead of RAGBench/HotpotQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saadkhalid/Projects/agents/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenRouter client with the OpenAI SDK\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "# Model being evaluated\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "\n",
    "# Scoring model\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "# RAG retrieval settings\n",
    "TOP_K = 5\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "# Embedding function for ChromaDB (uses OpenAI directly, not OpenRouter)\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BEIR/NQ queries and relevance judgments...\n",
      "Selected 50 queries, 61 gold documents\n",
      "Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\n",
      "Locating gold documents in corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2681468/2681468 [00:01<00:00, 2206177.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61/61 gold documents\n",
      "Corpus: 10000 docs (61 gold + 9939 distractors)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit! Reusing collection 'nq_10000_b1bf36b34ec3' (10,000 docs, no re-embedding needed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the eval model with retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        retrieved_docs: List of document texts retrieved from the collection\n",
    "\n",
    "    Returns:\n",
    "        The generated answer as a string\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\n",
    "Your task is to:\n",
    "1. Carefully read all provided documents\n",
    "2. Find the information needed to answer the question\n",
    "3. Provide a clear, concise answer based ONLY on the documents\n",
    "\n",
    "If the answer cannot be found in the documents, say so explicitly.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question based on the provided documents.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=EVAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "def score_answer(\n",
    "    question: str,\n",
    "    documents: list[str],\n",
    "    generated_answer: str,\n",
    ") -> tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Score the generated answer using the scoring model.\n",
    "\n",
    "    Args:\n",
    "        question: The original question\n",
    "        documents: The retrieved documents used to generate the answer\n",
    "        generated_answer: The answer generated by the eval model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (score out of 100, explanation)\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\n",
    "Evaluate the answer on these criteria:\n",
    "1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n",
    "2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n",
    "3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n",
    "4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "{\n",
    "    \"score\": <integer from 0-100>,\n",
    "    \"reasoning\": \"<brief explanation of the score>\"\n",
    "}\"\"\"\n",
    "\n",
    "    eval_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=SCORING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score_data = json.loads(response.choices[0].message.content or \"{}\")\n",
    "        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return 0, \"Error parsing score response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate a single query against the shared collection.\n",
    "\n",
    "    Retrieves top-k documents, computes Recall@K against ground-truth\n",
    "    relevance labels, generates an answer, and scores it.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    # Retrieve from the shared collection\n",
    "    results = collection.query(query_texts=[question], n_results=top_k)\n",
    "\n",
    "    retrieved_ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "    retrieved_docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "    # Compute Recall@K: what fraction of gold docs did we find?\n",
    "    hits = len(gold_ids & set(retrieved_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    # Generate and score\n",
    "    generated_answer = generate_answer(question, retrieved_docs)\n",
    "    score, reasoning = score_answer(question, retrieved_docs, generated_answer)\n",
    "\n",
    "    result = {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "    return example_index, result\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries: list[dict],\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    "    max_workers: int = 8,\n",
    ") -> dict:\n",
    "    \"\"\"Run evaluation on all queries using parallel workers.\"\"\"\n",
    "    eval_size = len(eval_queries)\n",
    "\n",
    "    print(f\"Running evaluation on {eval_size} queries with \"\n",
    "          f\"{max_workers} parallel workers...\\n\")\n",
    "\n",
    "    results_by_index: dict[int, dict] = {}\n",
    "    scores: list[int] = []\n",
    "    recalls: list[float] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(\n",
    "                evaluate_single_example,\n",
    "                eval_queries[i], i, collection, qrels, top_k\n",
    "            ): i\n",
    "            for i in range(eval_size)\n",
    "        }\n",
    "\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                completed += 1\n",
    "\n",
    "                # Print progress\n",
    "                print(\n",
    "                    f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question: {result['question'][:80]}...\")\n",
    "                print(f\"  Recall@{top_k}: {result['recall_at_k']:.2f} \"\n",
    "                      f\"({result['gold_docs_found']}/{result['gold_docs_total']} gold)\")\n",
    "                print(f\"  Score: {result['score']}/100\")\n",
    "                print(f\"  Reasoning: {result['scoring_reasoning']}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    results = [results_by_index[i]\n",
    "               for i in range(eval_size) if i in results_by_index]\n",
    "\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL,\n",
    "        \"scoring_model\": SCORING_MODEL,\n",
    "        \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(),\n",
    "        \"top_k\": top_k,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(avg_score, 2),\n",
    "        \"avg_recall_at_k\": round(avg_recall, 4),\n",
    "        \"individual_scores\": scores,\n",
    "        \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 50 queries with 8 parallel workers...\n",
      "\n",
      "[1/50] Example 8\n",
      "  Question: in order to prove disparate impact you first must establish...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and directly quotes the relevant criteria for proving disparate impact as defined in Document 1. It is complete, faithful to the text, and clearly stated.\n",
      "\n",
      "[2/50] Example 4\n",
      "  Question: who is the leader of the ontario pc party...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: \n",
      "\n",
      "[3/50] Example 3\n",
      "  Question: who sings love will keep us alive by the eagles...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate, complete, and faithful to the provided documents. Document 1 explicitly states that the song was performed by the Eagles with lead vocals by bassist Timothy B. Schmit.\n",
      "\n",
      "[4/50] Example 5\n",
      "  Question: nitty gritty dirt band fishin in the dark album...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1. It accurately identifies the album 'Hold On', is faithful to the text, and provides the information clearly and concisely.\n",
      "\n",
      "[5/50] Example 9\n",
      "  Question: where do characters live in this is us...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to Document 1, which contains the relevant information for the show 'This Is Us'. It correctly identifies the locations for all the main characters in both the present and past timelines mentioned in the text.\n",
      "\n",
      "[6/50] Example 2\n",
      "  Question: how many episodes are in chicago fire season 4...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on the provided documents. It correctly identifies the episode count for the specific season mentioned in Document 1 and cites the source correctly. The information is clear and concise.\n",
      "\n",
      "[7/50] Example 6\n",
      "  Question: who were the three elves who got rings...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to Document 2. It accurately identifies the three elves (Gil-galad, Círdan, and Galadriel) and their respective rings, while also noting the slight variation in the accounts found within the document.\n",
      "\n",
      "[8/50] Example 1\n",
      "  Question: what is non controlling interest on balance sheet...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate and faithful to the provided documents. It correctly identifies the definition of non-controlling interest from Document 1 and explains its specific treatment on the balance sheet using information from Document 2. It is clear, concise, and fully answers the question.\n",
      "\n",
      "[9/50] Example 12\n",
      "  Question: who recorded i can't help falling in love with you...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct, complete, and faithful to the provided documents. It correctly identifies the original artist (Elvis Presley) and lists the other artists mentioned in Document 1 (Tom Smothers, A-Teens, and UB40). It is also clear and well-structured.\n",
      "\n",
      "[10/50] Example 11\n",
      "  Question: when is the new tappan zee bridge going to be finished...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct, complete, and faithful to the provided documents. It correctly identifies the completion date mentioned in Document 1 and provides the necessary context regarding the already opened north span to fully answer the question.\n",
      "\n",
      "[11/50] Example 13\n",
      "  Question: who plays the doc in back to the future...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, complete, and faithful to the documents. It correctly identifies Christopher Lloyd as the actor and provides the specific document references for verification.\n",
      "\n",
      "[12/50] Example 7\n",
      "  Question: converting stereo signal to mono signal is called...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and faithful to the provided documents. It correctly identifies that Document 1 describes the process of creating a dual mono signal from a stereo source using panning, while also accurately noting that a single-word term for the conversion is not provided in the text.\n",
      "\n",
      "[13/50] Example 17\n",
      "  Question: what was the name of atom bomb dropped by usa on hiroshima...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, faithful to the provided documents, and clear. Document 1 explicitly states that 'one of its B-29s dropped a Little Boy uranium gun-type bomb on Hiroshima'.\n",
      "\n",
      "[14/50] Example 15\n",
      "  Question: who has been chosen as the brand ambassador of the campaign 'beti bachao-beti pa...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and directly supported by Document 1. It correctly identifies Sakshi Malik as the brand ambassador for BBBP (Beti Bachao-Beti Padhao) and includes the specific date mentioned in the text. The response is clear, faithful to the provided source, and fully answers the question.\n",
      "\n",
      "[15/50] Example 14\n",
      "  Question: when did they stop cigarette advertising on television...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1. It provides the exact date mentioned in the text (January 2, 1971), identifies the specific legislation responsible for the ban, and is clear and concise.\n",
      "\n",
      "[16/50] Example 20\n",
      "  Question: where are the mitochondria located in the sperm...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and directly supported by multiple documents (Document 1 and Document 2). It correctly identifies the location (midpiece) and provides the specific detail regarding its position relative to the head and its function mentioned in the text.\n",
      "\n",
      "[17/50] Example 21\n",
      "  Question: how many lines of symmetry are there in a equilateral triangle...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, complete, and faithful to the provided documents. It directly extracts the information from Document 1 and presents it clearly.\n",
      "\n",
      "[18/50] Example 23\n",
      "  Question: latest season on keeping up with the kardashians...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to the provided documents. It correctly identifies the fourteenth season as the latest one mentioned in the text (Document 1) and provides the context that those seasons concluded as of December 10, 2017.\n",
      "\n",
      "[19/50] Example 10\n",
      "  Question: who makes the decisions about what to produce in a market economy...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[20/50] Example 22\n",
      "  Question: how many seasons of the oc are there...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1, which identifies the season airing in 2006-2007 as the 'fourth and final season.' The answer is faithful to the text, complete, and clearly written.\n",
      "\n",
      "[21/50] Example 18\n",
      "  Question: when did the us take over wake island...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and fully supported by the provided documents. It correctly identifies the date of formal possession (January 17, 1899) and the preceding event of the flag raising (July 4, 1898), providing all relevant details from Documents 1 and 2.\n",
      "\n",
      "[22/50] Example 16\n",
      "  Question: how many seasons of prison break are on netflix...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to the documents. Document 1 mentions that Prison Break is available on Netflix but does not provide a season count. The answer correctly notes that while other formats (DVD/Blu-ray) have specific season counts mentioned (5 and 8 respectively), the streaming count for Netflix is missing from the provided text.\n",
      "\n",
      "[23/50] Example 25\n",
      "  Question: who were farmers who kept a small portion of their crops & gave the rest to the ...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and directly supported by Document 2, which defines sharecropping as an arrangement where tenants give a share of crops to the landowner. It correctly identifies the term and provides relevant examples found in the text.\n",
      "\n",
      "[24/50] Example 19\n",
      "  Question: when did american two party system began to emerge...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and directly supported by Document 1, which identifies the Federalist Party and the Democratic-Republican Party as the only significant parties in the First Party System. The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[25/50] Example 28\n",
      "  Question: where are they building the new raiders stadium...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and directly cites the specific location details provided in Document 1, including the city, acreage, nearby landmarks, and specific street boundaries.\n",
      "\n",
      "[26/50] Example 24\n",
      "  Question: how dose the poet present death as a voyage in crossing the bar...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate and faithful to the provided documents. It correctly identifies the extended metaphor of 'crossing the bar' as a serene voyage from life to death and explains the role of the 'Pilot' using the specific details and quotes found in Document 1. The response is clear, well-structured, and fully addresses the question.\n",
      "\n",
      "[27/50] Example 29\n",
      "  Question: what event provoked congress to propose the eleventh amendment and the states to...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct and faithful to the provided documents. It correctly identifies the Supreme Court case Chisholm v. Georgia (1793) as the catalyst for the amendment, explains the legal ruling of that case, and mentions the amendment's purpose in clarifying Article III, Section 2. The clarity and completeness are excellent.\n",
      "\n",
      "[28/50] Example 31\n",
      "  Question: what is the meaning of the name comanche...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on the provided documents. Document 1 explicitly states that the name 'Comanche' is from the Ute name 'kɨmantsi', meaning 'enemy'. The answer is concise, clear, and stays faithful to the text without any hallucinations.\n",
      "\n",
      "[29/50] Example 27\n",
      "  Question: 2. what are the reasons states impose protectionists policies on other countries...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[30/50] Example 26\n",
      "  Question: what is pumped up kicks the song about...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[31/50] Example 32\n",
      "  Question: who do you meet at the gates of heaven...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to the provided documents. It correctly identifies Saint Peter as the figure met at the gates based on Document 1 and includes the relevant details provided in that document.\n",
      "\n",
      "[32/50] Example 34\n",
      "  Question: is spain the second largest country in europe...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate, faithful to the provided documents, and clear. It correctly identifies Spain's rank in Europe (fourth) versus its rank in Western Europe (second) as specified in Document 1.\n",
      "\n",
      "[33/50] Example 33\n",
      "  Question: what are bulls used for on a farm...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies all the uses of bulls/male cattle mentioned in the documents: breeding/herd maintenance, artificial insemination, meat production, and work (oxen). It sticks strictly to the provided text and is well-organized.\n",
      "\n",
      "[34/50] Example 36\n",
      "  Question: what is the meaning of the dragon boat festival...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct, complete, and faithful to the provided documents. It accurately identifies the meaning (commemorating fealty and filial piety) and provides the relevant context (alternative names and timing) found in Document 1. The clarity is excellent.\n",
      "\n",
      "[35/50] Example 30\n",
      "  Question: does joe die in the purge election year...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. Document 2 explicitly states that Joe is fatally wounded after shooting Harmon James and dies after asking Marcos to take care of his store. The answer is clear and directly addresses the question.\n",
      "\n",
      "[36/50] Example 35\n",
      "  Question: difference between single layer perceptron and multilayer perceptron...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies the differences between a true (single) perceptron and an MLP based solely on the provided Document 1. It correctly distinguishes them based on structure, activation functions, and functionality (classification vs. regression). The answer is clear, well-organized, and avoids any information not found in the source text.\n",
      "\n",
      "[37/50] Example 40\n",
      "  Question: royal society for the protection of birds number of members...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1. It provides the exact figure mentioned in the text, identifies the source, and includes relevant detail regarding youth members. It is concise and faithful to the provided snippets.\n",
      "\n",
      "[38/50] Example 37\n",
      "  Question: who is edmund on days of our lives...\n",
      "  Recall@5: 0.00 (0/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct. I have reviewed all five provided documents, and while they mention several characters (Kristen DiMera, Bill Hayes, Sami Brady, Lucas, EJ, Sydney DiMera, Theo Carver, etc.), there is no mention of a character named Edmund. The model correctly identified that the information is missing.\n",
      "\n",
      "[39/50] Example 41\n",
      "  Question: when was theme from a summer place released...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate based on the provided documents. It correctly identifies the release date (September 1959) and the specific version mentioned (Percy Faith), while citing information only found in the text.\n",
      "\n",
      "[40/50] Example 39\n",
      "  Question: what degree is a crock pot on low...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct and faithful to the provided documents. Document 1 specifies temperatures for the 'warming' setting (71–74 °C) but does not provide a specific temperature for the 'low' setting. The answer correctly identifies this distinction and avoids making outside assumptions.\n",
      "\n",
      "[41/50] Example 45\n",
      "  Question: who sings war don't let me down...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and directly uses the information provided in Document 1. It correctly identifies both the production duo and the featured vocalist, and it remains faithful to the text without including outside information.\n",
      "\n",
      "[42/50] Example 43\n",
      "  Question: when did athens emerges as wealthiest greek city state...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct and directly supported by Document 1. It accurately identifies the time period (late 6th century BCE) and provides the supporting detail regarding the discovery of silver as mentioned in the text. The answer is clear, faithful to the documents, and complete.\n",
      "\n",
      "[43/50] Example 38\n",
      "  Question: i want to be with you everywhere song...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer identifies the correct song from the provided documents and includes all relevant details such as the artist, writer, album, release dates, and chart performance. It is faithful to the text and clearly organized.\n",
      "\n",
      "[44/50] Example 42\n",
      "  Question: when does jenny humphrey come back to gossip girl...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to the provided documents. It correctly identifies the timeline (final episode, five-year time jump) and the context (Dan's wedding) as specified in Document 1.\n",
      "\n",
      "[45/50] Example 47\n",
      "  Question: who plays v on orange is the new black...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely correct and faithful to Document 2. It accurately identifies Lorraine Toussaint as the actor who plays Yvonne 'Vee' Parker, and it provides the specific context from the text to support its claim.\n",
      "\n",
      "[46/50] Example 46\n",
      "  Question: who invented the frisbee and how did it get its name...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies Fred Morrison as the inventor based on the provided documents. It correctly explains the timeline of the name change from 'Pluto Platter' to 'Frisbee' and identifies the source of the name (college students in the Northeast). The answer is faithful to the text and includes a helpful note clarifying that the documents do not provide the etymology of the word 'Frisbee' itself.\n",
      "\n",
      "[47/50] Example 50\n",
      "  Question: when do willow and tara get back together...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and directly uses the information provided in Document 1. It correctly identifies the episode ('Seeing Red') where the reconciliation occurs and provides the context of the preceding episode ('Entropy') as mentioned in the text. The answer is clear, concise, and faithful to the source material.\n",
      "\n",
      "[48/50] Example 48\n",
      "  Question: how long prime minister stay in office canada...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately reflects the information in Document 1, distinguishing between the lack of a fixed term for the individual and the statutory four-year cycle for elections. It uses only the provided documents and is very clearly written.\n",
      "\n",
      "[49/50] Example 44\n",
      "  Question: why does king from tekken wear a mask...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: \n",
      "\n",
      "[50/50] Example 49\n",
      "  Question: where was the movie jeremiah johnson filmed at...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and fully supported by the provided documents (specifically Documents 2 and 3). It correctly identifies Utah as the state and lists the specific locations mentioned in the text without including outside information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Model evaluated:  moonshotai/kimi-k2.5\n",
      "Scoring model:    google/gemini-3-flash-preview\n",
      "Dataset:          BeIR/NQ\n",
      "Corpus size:      10,000 documents\n",
      "Top-K:            5\n",
      "Queries evaluated: 50\n",
      "\n",
      "Avg Recall@5:    0.9800\n",
      "Overall Score:    90.0/100\n",
      "\n",
      "Score Distribution:\n",
      "  90-100: 45 examples\n",
      "  80-89: 0 examples\n",
      "  70-79: 0 examples\n",
      "  60-69: 0 examples\n",
      "  below-60: 5 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to evals/1.4_large_corpus_rag.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.4_large_corpus_rag.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
