{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8 Agentic RAG\n",
    "\n",
    "All previous approaches use a fixed pipeline: retrieve once, answer once. Agentic RAG adds a feedback loop: the model evaluates its own retrieval results and decides whether to try again with a different query.\n",
    "\n",
    "The loop:\n",
    "\n",
    "1. Retrieve top-K documents\n",
    "2. LLM evaluates: \"Do these documents contain enough information to answer?\"\n",
    "3. If yes: generate the answer\n",
    "4. If no: LLM reformulates the query and goes back to step 1\n",
    "5. After MAX_RETRIES, answer with whatever we have\n",
    "\n",
    "This is the first technique in our progression where the model makes its own decisions about the retrieval process. It's not just transforming the query (1.5) or re-scoring results (1.6) -- it's deciding _whether the results are good enough_ and _what to do if they aren't_.\n",
    "\n",
    "Same corpus, same collection, same scoring as 1.4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saadkhalid/Projects/agents/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "# for retrieval evaluation + query reformulation\n",
    "AGENT_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "TOP_K = 5\n",
    "MAX_RETRIES = 3    # max retrieval attempts before giving up\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BEIR/NQ queries and relevance judgments...\n",
      "Selected 50 queries, 61 gold documents\n",
      "Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\n",
      "Locating gold documents in corpus...\n",
      "Found 61/61 gold documents\n",
      "Corpus: 10000 docs (61 gold + 9939 distractors)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit! Reusing collection 'nq_10000_b1bf36b34ec3' (10,000 docs, no re-embedding needed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Retrieval Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    question: str,\n",
    "    doc_texts: list[str],\n",
    ") -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Ask the agent model whether the retrieved documents are sufficient\n",
    "    to answer the question. If not, it suggests a reformulated query.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (is_sufficient, reformulated_query)\n",
    "        - is_sufficient: True if docs are good enough to answer\n",
    "        - reformulated_query: new search query if not sufficient\n",
    "    \"\"\"\n",
    "    doc_list = \"\\n\\n\".join(\n",
    "        f\"[Doc {i+1}] {text[:300]}\" for i, text in enumerate(doc_texts)\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AGENT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are a retrieval quality evaluator. Given a question and retrieved documents, \"\n",
    "                \"decide if the documents contain enough information to answer the question.\\n\\n\"\n",
    "                \"Respond with ONLY a JSON object:\\n\"\n",
    "                '{\\n'\n",
    "                '  \"sufficient\": true/false,\\n'\n",
    "                '  \"reasoning\": \"brief explanation\",\\n'\n",
    "                '  \"reformulated_query\": \"new search query if not sufficient\"\\n'\n",
    "                '}\\n\\n'\n",
    "                \"Set sufficient=true if ANY document contains relevant information. \"\n",
    "                \"If not sufficient, write a reformulated_query that approaches \"\n",
    "                \"the topic from a different angle -- use different keywords, \"\n",
    "                \"related concepts, or alternative phrasings.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nRetrieved Documents:\\n{doc_list}\"},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        content = response.choices[0].message.content or \"{}\"\n",
    "        if \"```\" in content:\n",
    "            content = content.split(\"```\")[1]\n",
    "            if content.startswith(\"json\"):\n",
    "                content = content[4:]\n",
    "        data = json.loads(content.strip())\n",
    "        return data.get(\"sufficient\", True), data.get(\"reformulated_query\", question)\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return True, question  # default: accept what we have\n",
    "\n",
    "\n",
    "def agentic_retrieve(\n",
    "    question: str,\n",
    "    collection: chromadb.Collection,\n",
    "    top_k: int,\n",
    "    max_retries: int,\n",
    ") -> tuple[list[str], list[str], int, list[str]]:\n",
    "    \"\"\"\n",
    "    Retrieve with a feedback loop: retrieve, evaluate, reformulate, retry.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (final_ids, final_docs, attempts_used, queries_tried)\n",
    "    \"\"\"\n",
    "    current_query = question\n",
    "    queries_tried = [current_query]\n",
    "    best_ids, best_docs = [], []\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        results = collection.query(\n",
    "            query_texts=[current_query], n_results=top_k)\n",
    "        retrieved_ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "        retrieved_docs = results[\"documents\"][0] if results[\"documents\"] else [\n",
    "        ]\n",
    "\n",
    "        # Keep track of best results (union of all retrieved)\n",
    "        if not best_ids:\n",
    "            best_ids, best_docs = retrieved_ids, retrieved_docs\n",
    "\n",
    "        # Ask the agent: are these results good enough?\n",
    "        if attempt < max_retries:  # don't evaluate on last attempt\n",
    "            sufficient, reformulated = evaluate_retrieval(\n",
    "                question, retrieved_docs)\n",
    "            if sufficient:\n",
    "                return retrieved_ids, retrieved_docs, attempt, queries_tried\n",
    "            # Not sufficient -- try the reformulated query\n",
    "            current_query = reformulated\n",
    "            queries_tried.append(current_query)\n",
    "            # Merge new results with previous (deduplicated)\n",
    "            seen = set(best_ids)\n",
    "            for rid, rdoc in zip(retrieved_ids, retrieved_docs):\n",
    "                if rid not in seen:\n",
    "                    best_ids.append(rid)\n",
    "                    best_docs.append(rdoc)\n",
    "                    seen.add(rid)\n",
    "        else:\n",
    "            # Last attempt: merge and return top_k\n",
    "            seen = set(best_ids)\n",
    "            for rid, rdoc in zip(retrieved_ids, retrieved_docs):\n",
    "                if rid not in seen:\n",
    "                    best_ids.append(rid)\n",
    "                    best_docs.append(rdoc)\n",
    "                    seen.add(rid)\n",
    "\n",
    "    return best_ids[:top_k], best_docs[:top_k], max_retries, queries_tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the eval model with retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        retrieved_docs: List of document texts retrieved from the collection\n",
    "\n",
    "    Returns:\n",
    "        The generated answer as a string\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\n",
    "Your task is to:\n",
    "1. Carefully read all provided documents\n",
    "2. Find the information needed to answer the question\n",
    "3. Provide a clear, concise answer based ONLY on the documents\n",
    "\n",
    "If the answer cannot be found in the documents, say so explicitly.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question based on the provided documents.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=EVAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "def score_answer(\n",
    "    question: str,\n",
    "    documents: list[str],\n",
    "    generated_answer: str,\n",
    ") -> tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Score the generated answer using the scoring model.\n",
    "\n",
    "    Args:\n",
    "        question: The original question\n",
    "        documents: The retrieved documents used to generate the answer\n",
    "        generated_answer: The answer generated by the eval model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (score out of 100, explanation)\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\n",
    "Evaluate the answer on these criteria:\n",
    "1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n",
    "2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n",
    "3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n",
    "4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "{\n",
    "    \"score\": <integer from 0-100>,\n",
    "    \"reasoning\": \"<brief explanation of the score>\"\n",
    "}\"\"\"\n",
    "\n",
    "    eval_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=SCORING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score_data = json.loads(response.choices[0].message.content or \"{}\")\n",
    "        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return 0, \"Error parsing score response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate using agentic retrieval: retrieve, evaluate, reformulate, retry.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    retrieved_ids, retrieved_docs, attempts, queries_tried = agentic_retrieve(\n",
    "        question, collection, top_k, MAX_RETRIES\n",
    "    )\n",
    "\n",
    "    hits = len(gold_ids & set(retrieved_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    generated_answer = generate_answer(question, retrieved_docs)\n",
    "    score, reasoning = score_answer(question, retrieved_docs, generated_answer)\n",
    "\n",
    "    return example_index, {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"queries_tried\": queries_tried,\n",
    "        \"retrieval_attempts\": attempts,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries, collection, qrels, top_k, max_workers=8\n",
    ") -> dict:\n",
    "    eval_size = len(eval_queries)\n",
    "    print(\n",
    "        f\"Running evaluation on {eval_size} queries with {max_workers} parallel workers...\\n\")\n",
    "    results_by_index, scores, recalls, all_attempts = {}, [], [], []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(evaluate_single_example, eval_queries[i], i, collection, qrels, top_k): i\n",
    "            for i in range(eval_size)\n",
    "        }\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                all_attempts.append(result[\"retrieval_attempts\"])\n",
    "                completed += 1\n",
    "                print(f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question: {result['question'][:80]}...\")\n",
    "                print(\n",
    "                    f\"  Attempts: {result['retrieval_attempts']} | Queries: {result['queries_tried']}\")\n",
    "                print(\n",
    "                    f\"  Recall@{top_k}: {result['recall_at_k']:.2f} ({result['gold_docs_found']}/{result['gold_docs_total']} gold)\")\n",
    "                print(f\"  Score: {result['score']}/100\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    ordered = [results_by_index[i]\n",
    "               for i in range(eval_size) if i in results_by_index]\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL, \"scoring_model\": SCORING_MODEL,\n",
    "        \"agent_model\": AGENT_MODEL, \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(), \"top_k\": top_k,\n",
    "        \"max_retries\": MAX_RETRIES,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(sum(scores)/len(scores), 2) if scores else 0,\n",
    "        \"avg_recall_at_k\": round(sum(recalls)/len(recalls), 4) if recalls else 0,\n",
    "        \"avg_retrieval_attempts\": round(sum(all_attempts)/len(all_attempts), 2) if all_attempts else 0,\n",
    "        \"individual_scores\": scores, \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": ordered,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 50 queries with 8 parallel workers...\n",
      "\n",
      "[1/50] Example 3\n",
      "  Question: who sings love will keep us alive by the eagles...\n",
      "  Attempts: 1 | Queries: ['who sings love will keep us alive by the eagles']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[2/50] Example 1\n",
      "  Question: what is non controlling interest on balance sheet...\n",
      "  Attempts: 1 | Queries: ['what is non controlling interest on balance sheet']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[3/50] Example 4\n",
      "  Question: who is the leader of the ontario pc party...\n",
      "  Attempts: 2 | Queries: ['who is the leader of the ontario pc party', 'current leader of the Progressive Conservative Party of Ontario 2024']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[4/50] Example 8\n",
      "  Question: in order to prove disparate impact you first must establish...\n",
      "  Attempts: 1 | Queries: ['in order to prove disparate impact you first must establish']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[5/50] Example 2\n",
      "  Question: how many episodes are in chicago fire season 4...\n",
      "  Attempts: 3 | Queries: ['how many episodes are in chicago fire season 4', 'number of episodes in Chicago Fire season 4', 'Chicago Fire season 4 episode list count']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[6/50] Example 6\n",
      "  Question: who were the three elves who got rings...\n",
      "  Attempts: 1 | Queries: ['who were the three elves who got rings']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[7/50] Example 11\n",
      "  Question: when is the new tappan zee bridge going to be finished...\n",
      "  Attempts: 3 | Queries: ['when is the new tappan zee bridge going to be finished', 'Governor Mario M. Cuomo Bridge completion date south span opening', 'Governor Mario M. Cuomo Bridge south span completion date']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[8/50] Example 10\n",
      "  Question: who makes the decisions about what to produce in a market economy...\n",
      "  Attempts: 1 | Queries: ['who makes the decisions about what to produce in a market economy']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[9/50] Example 13\n",
      "  Question: who plays the doc in back to the future...\n",
      "  Attempts: 1 | Queries: ['who plays the doc in back to the future']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[10/50] Example 5\n",
      "  Question: nitty gritty dirt band fishin in the dark album...\n",
      "  Attempts: 1 | Queries: ['nitty gritty dirt band fishin in the dark album']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[11/50] Example 15\n",
      "  Question: who has been chosen as the brand ambassador of the campaign 'beti bachao-beti pa...\n",
      "  Attempts: 1 | Queries: [\"who has been chosen as the brand ambassador of the campaign 'beti bachao-beti padhao\"]\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[12/50] Example 12\n",
      "  Question: who recorded i can't help falling in love with you...\n",
      "  Attempts: 1 | Queries: [\"who recorded i can't help falling in love with you\"]\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[13/50] Example 9\n",
      "  Question: where do characters live in this is us...\n",
      "  Attempts: 3 | Queries: ['where do characters live in this is us', 'where are the primary settings for the Pearson family in This Is Us', 'What are the primary filming locations and settings for the Pearson family in This Is Us?']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[14/50] Example 20\n",
      "  Question: where are the mitochondria located in the sperm...\n",
      "  Attempts: 1 | Queries: ['where are the mitochondria located in the sperm']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[15/50] Example 19\n",
      "  Question: when did american two party system began to emerge...\n",
      "  Attempts: 1 | Queries: ['when did american two party system began to emerge']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[16/50] Example 16\n",
      "  Question: how many seasons of prison break are on netflix...\n",
      "  Attempts: 3 | Queries: ['how many seasons of prison break are on netflix', 'is prison break currently streaming on netflix and which seasons are available', 'is Prison Break available on Netflix and how many seasons are streaming']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[17/50] Example 18\n",
      "  Question: when did the us take over wake island...\n",
      "  Attempts: 1 | Queries: ['when did the us take over wake island']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[18/50] Example 14\n",
      "  Question: when did they stop cigarette advertising on television...\n",
      "  Attempts: 3 | Queries: ['when did they stop cigarette advertising on television', 'date of cigarette television advertising ban United States Public Health Cigarette Smoking Act', 'date of cigarette television advertising ban United States Public Health Cigarette Smoking Act']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[19/50] Example 17\n",
      "  Question: what was the name of atom bomb dropped by usa on hiroshima...\n",
      "  Attempts: 3 | Queries: ['what was the name of atom bomb dropped by usa on hiroshima', 'name of atomic bomb dropped on Hiroshima August 6 1945', 'name of atomic bomb dropped on Hiroshima August 6 1945']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[20/50] Example 25\n",
      "  Question: who were farmers who kept a small portion of their crops & gave the rest to the ...\n",
      "  Attempts: 1 | Queries: ['who were farmers who kept a small portion of their crops & gave the rest to the landowners']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[21/50] Example 7\n",
      "  Question: converting stereo signal to mono signal is called...\n",
      "  Attempts: 1 | Queries: ['converting stereo signal to mono signal is called']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 85/100\n",
      "\n",
      "[22/50] Example 21\n",
      "  Question: how many lines of symmetry are there in a equilateral triangle...\n",
      "  Attempts: 1 | Queries: ['how many lines of symmetry are there in a equilateral triangle']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[23/50] Example 22\n",
      "  Question: how many seasons of the oc are there...\n",
      "  Attempts: 1 | Queries: ['how many seasons of the oc are there']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[24/50] Example 24\n",
      "  Question: how dose the poet present death as a voyage in crossing the bar...\n",
      "  Attempts: 1 | Queries: ['how dose the poet present death as a voyage in crossing the bar']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[25/50] Example 31\n",
      "  Question: what is the meaning of the name comanche...\n",
      "  Attempts: 1 | Queries: ['what is the meaning of the name comanche']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[26/50] Example 28\n",
      "  Question: where are they building the new raiders stadium...\n",
      "  Attempts: 1 | Queries: ['where are they building the new raiders stadium']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[27/50] Example 30\n",
      "  Question: does joe die in the purge election year...\n",
      "  Attempts: 3 | Queries: ['does joe die in the purge election year', 'death of Joe Dixon The Purge: Election Year ending', 'Joe Dixon fate The Purge Election Year ending']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[28/50] Example 33\n",
      "  Question: what are bulls used for on a farm...\n",
      "  Attempts: 1 | Queries: ['what are bulls used for on a farm']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[29/50] Example 32\n",
      "  Question: who do you meet at the gates of heaven...\n",
      "  Attempts: 1 | Queries: ['who do you meet at the gates of heaven']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[30/50] Example 23\n",
      "  Question: latest season on keeping up with the kardashians...\n",
      "  Attempts: 1 | Queries: ['latest season on keeping up with the kardashians']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[31/50] Example 36\n",
      "  Question: what is the meaning of the dragon boat festival...\n",
      "  Attempts: 1 | Queries: ['what is the meaning of the dragon boat festival']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[32/50] Example 29\n",
      "  Question: what event provoked congress to propose the eleventh amendment and the states to...\n",
      "  Attempts: 1 | Queries: ['what event provoked congress to propose the eleventh amendment and the states to ratify it']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[33/50] Example 37\n",
      "  Question: who is edmund on days of our lives...\n",
      "  Attempts: 3 | Queries: ['who is edmund on days of our lives', 'Edmund Crumb Days of Our Lives character biography', 'Edmund Days of our Lives character biography actor']\n",
      "  Recall@5: 0.00 (0/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[34/50] Example 26\n",
      "  Question: what is pumped up kicks the song about...\n",
      "  Attempts: 1 | Queries: ['what is pumped up kicks the song about']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[35/50] Example 38\n",
      "  Question: i want to be with you everywhere song...\n",
      "  Attempts: 1 | Queries: ['i want to be with you everywhere song']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[36/50] Example 27\n",
      "  Question: 2. what are the reasons states impose protectionists policies on other countries...\n",
      "  Attempts: 1 | Queries: ['2. what are the reasons states impose protectionists policies on other countries']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[37/50] Example 35\n",
      "  Question: difference between single layer perceptron and multilayer perceptron...\n",
      "  Attempts: 3 | Queries: ['difference between single layer perceptron and multilayer perceptron', 'comparison of architecture and capabilities of single-layer vs multi-layer perceptrons', 'comparison of single layer perceptron vs multilayer perceptron architecture and capabilities']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[38/50] Example 39\n",
      "  Question: what degree is a crock pot on low...\n",
      "  Attempts: 3 | Queries: ['what degree is a crock pot on low', 'average temperature of slow cooker low setting in degrees', 'average temperature of slow cooker low setting in Fahrenheit and Celsius']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[39/50] Example 42\n",
      "  Question: when does jenny humphrey come back to gossip girl...\n",
      "  Attempts: 1 | Queries: ['when does jenny humphrey come back to gossip girl']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[40/50] Example 45\n",
      "  Question: who sings war don't let me down...\n",
      "  Attempts: 1 | Queries: [\"who sings war don't let me down\"]\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[41/50] Example 34\n",
      "  Question: is spain the second largest country in europe...\n",
      "  Attempts: 1 | Queries: ['is spain the second largest country in europe']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[42/50] Example 41\n",
      "  Question: when was theme from a summer place released...\n",
      "  Attempts: 1 | Queries: ['when was theme from a summer place released']\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[43/50] Example 46\n",
      "  Question: who invented the frisbee and how did it get its name...\n",
      "  Attempts: 3 | Queries: ['who invented the frisbee and how did it get its name', 'origin of the name frisbee and inventor Walter Fredrick Morrison', 'origin of the name Frisbee and Walter Frederick Morrison history']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[44/50] Example 50\n",
      "  Question: when do willow and tara get back together...\n",
      "  Attempts: 1 | Queries: ['when do willow and tara get back together']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[45/50] Example 48\n",
      "  Question: how long prime minister stay in office canada...\n",
      "  Attempts: 1 | Queries: ['how long prime minister stay in office canada']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[46/50] Example 44\n",
      "  Question: why does king from tekken wear a mask...\n",
      "  Attempts: 3 | Queries: ['why does king from tekken wear a mask', \"origin of King's jaguar mask Tekken lore Fray Tormenta\", 'origin of King Tekken jaguar mask and wrestling persona']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[47/50] Example 40\n",
      "  Question: royal society for the protection of birds number of members...\n",
      "  Attempts: 1 | Queries: ['royal society for the protection of birds number of members']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[48/50] Example 43\n",
      "  Question: when did athens emerges as wealthiest greek city state...\n",
      "  Attempts: 1 | Queries: ['when did athens emerges as wealthiest greek city state']\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"Agent model  :    {results['agent_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.8_agentic_rag.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
