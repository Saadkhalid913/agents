{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Large Corpus RAG with Query Rewriting\n",
    "\n",
    "Same setup as 1.4 -- BEIR/NQ dataset, shared 10K+ doc ChromaDB collection, Recall@K -- but with one change: before searching, we rewrite the user's question into a search-optimized query using an LLM.\n",
    "\n",
    "User questions are conversational (\"who sang that song in the movie?\"). Embedding search works better with keyword-rich, declarative statements (\"1993 film Aerosmith song soundtrack director\"). The rewrite model transforms one into the other.\n",
    "\n",
    "We search with the **rewritten** query but answer with the **original** question. The rewrite is optimized for retrieval, not comprehension. Everything else (corpus, collection, scoring) is identical to 1.4 so the comparison is controlled."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize OpenRouter client with the OpenAI SDK\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "# Model being evaluated\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "\n",
    "# Scoring model\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "# Query rewriting model\n",
    "REWRITE_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "# RAG retrieval settings\n",
    "TOP_K = 5\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "# Embedding function for ChromaDB (uses OpenAI directly, not OpenRouter)\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rewrite_query(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite a user question into a search-optimized query.\n",
    "\n",
    "    Conversational questions don't always match well against document\n",
    "    embeddings. This function asks an LLM to rewrite the question into\n",
    "    a keyword-rich, declarative form that's better suited for embedding\n",
    "    similarity search.\n",
    "\n",
    "    Args:\n",
    "        question: The original user question\n",
    "\n",
    "    Returns:\n",
    "        A search-optimized query string\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=REWRITE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are a search query optimizer. Given a user question, \"\n",
    "                \"rewrite it as a search query optimized for semantic similarity \"\n",
    "                \"search against a Wikipedia corpus. Follow these rules:\\n\"\n",
    "                \"1. Extract key entities, names, dates, and concepts\\n\"\n",
    "                \"2. Use declarative, keyword-rich phrasing instead of question form\\n\"\n",
    "                \"3. Expand abbreviations and add synonyms where helpful\\n\"\n",
    "                \"4. Remove filler words (who, what, how, please, etc.)\\n\"\n",
    "                \"5. Keep it concise -- one line, no explanation\\n\"\n",
    "                \"6. Output ONLY the rewritten query, nothing else\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n    \"\"\"\n    Generate an answer using the eval model with retrieved documents.\n\n    Args:\n        question: The question to answer\n        retrieved_docs: List of document texts retrieved from the collection\n\n    Returns:\n        The generated answer as a string\n    \"\"\"\n    context = \"\\n\\n\".join(\n        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n\n    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\nYour task is to:\n1. Carefully read all provided documents\n2. Find the information needed to answer the question\n3. Provide a clear, concise answer based ONLY on the documents\n\nIf the answer cannot be found in the documents, say so explicitly.\"\"\"\n\n    user_prompt = f\"\"\"Documents:\n{context}\n\nQuestion: {question}\n\nPlease answer the question based on the provided documents.\"\"\"\n\n    response = client.chat.completions.create(\n        model=EVAL_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        max_tokens=4096,\n    )\n\n    return response.choices[0].message.content or \"\"\n\n\ndef score_answer(\n    question: str,\n    documents: list[str],\n    generated_answer: str,\n) -> tuple[int, str]:\n    \"\"\"\n    Score the generated answer using the scoring model.\n\n    Args:\n        question: The original question\n        documents: The retrieved documents used to generate the answer\n        generated_answer: The answer generated by the eval model\n\n    Returns:\n        Tuple of (score out of 100, explanation)\n    \"\"\"\n    context = \"\\n\\n\".join(\n        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n\n    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\nEvaluate the answer on these criteria:\n1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n\nRespond with a JSON object containing:\n{\n    \"score\": <integer from 0-100>,\n    \"reasoning\": \"<brief explanation of the score>\"\n}\"\"\"\n\n    eval_prompt = f\"\"\"Documents:\n{context}\n\nQuestion: {question}\n\nGenerated Answer:\n{generated_answer}\"\"\"\n\n    response = client.chat.completions.create(\n        model=SCORING_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": eval_prompt},\n        ],\n        max_tokens=300,\n    )\n\n    try:\n        score_data = json.loads(response.choices[0].message.content or \"{}\")\n        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n    except json.JSONDecodeError:\n        return 0, \"Error parsing score response\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate a single query against the shared collection.\n",
    "\n",
    "    Rewrites the query for better retrieval, retrieves top-k documents,\n",
    "    computes Recall@K against ground-truth relevance labels, generates\n",
    "    an answer using the ORIGINAL question, and scores it.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    # Rewrite the question into a search-optimized query\n",
    "    rewritten = rewrite_query(question)\n",
    "\n",
    "    # Retrieve using the REWRITTEN query (not the original question)\n",
    "    results = collection.query(query_texts=[rewritten], n_results=top_k)\n",
    "\n",
    "    retrieved_ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "    retrieved_docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "    # Compute Recall@K: what fraction of gold docs did we find?\n",
    "    hits = len(gold_ids & set(retrieved_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    # Generate answer using the ORIGINAL question (not rewritten)\n",
    "    generated_answer = generate_answer(question, retrieved_docs)\n",
    "    score, reasoning = score_answer(question, retrieved_docs, generated_answer)\n",
    "\n",
    "    result = {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"rewritten_query\": rewritten,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "    return example_index, result\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries: list[dict],\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    "    max_workers: int = 8,\n",
    ") -> dict:\n",
    "    \"\"\"Run evaluation on all queries using parallel workers.\"\"\"\n",
    "    eval_size = len(eval_queries)\n",
    "\n",
    "    print(f\"Running evaluation on {eval_size} queries with \"\n",
    "          f\"{max_workers} parallel workers...\\n\")\n",
    "\n",
    "    results_by_index: dict[int, dict] = {}\n",
    "    scores: list[int] = []\n",
    "    recalls: list[float] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(\n",
    "                evaluate_single_example,\n",
    "                eval_queries[i], i, collection, qrels, top_k\n",
    "            ): i\n",
    "            for i in range(eval_size)\n",
    "        }\n",
    "\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                completed += 1\n",
    "\n",
    "                # Print progress\n",
    "                print(\n",
    "                    f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question:  {result['question'][:80]}...\")\n",
    "                print(f\"  Rewritten: {result['rewritten_query'][:80]}\")\n",
    "                print(f\"  Recall@{top_k}: {result['recall_at_k']:.2f} \"\n",
    "                      f\"({result['gold_docs_found']}/{result['gold_docs_total']} gold)\")\n",
    "                print(f\"  Score: {result['score']}/100\")\n",
    "                print(f\"  Reasoning: {result['scoring_reasoning']}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    results = [results_by_index[i]\n",
    "               for i in range(eval_size) if i in results_by_index]\n",
    "\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL,\n",
    "        \"scoring_model\": SCORING_MODEL,\n",
    "        \"rewrite_model\": REWRITE_MODEL,\n",
    "        \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(),\n",
    "        \"top_k\": top_k,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(avg_score, 2),\n",
    "        \"avg_recall_at_k\": round(avg_recall, 4),\n",
    "        \"individual_scores\": scores,\n",
    "        \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": results,\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, TOP_K)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"Rewrite model:    {results['rewrite_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.5_large_corpus_rag_with_query_rewriting.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}