{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Large Corpus RAG with HyDE\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** flips the retrieval problem: instead of embedding the question, we ask an LLM to generate a hypothetical answer, then embed *that* and search for real documents similar to it.\n",
    "\n",
    "Why does this work? A question like \"what is the capital of France?\" and a document containing \"Paris is the capital of France\" live in different parts of embedding space -- one is a question, the other is a statement. But a hypothetical answer (\"The capital of France is Paris\") is much closer to the real document.\n",
    "\n",
    "The hypothetical answer doesn't need to be correct. It just needs to *sound like* the kind of document that would contain the real answer. Even a wrong guess pulls the embedding toward the right neighborhood.\n",
    "\n",
    "Same corpus, same collection, same scoring as 1.4. The only change is what gets embedded for the search query."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "HYDE_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "TOP_K = 5\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "# Direct OpenAI client for embedding the hypothetical document\n",
    "openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDE: Hypothetical Document Generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_hypothetical_document(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a hypothetical document that would answer the question.\n",
    "\n",
    "    The output doesn't need to be factually correct -- it just needs to\n",
    "    sound like a Wikipedia passage about the topic, so its embedding\n",
    "    lands near real relevant documents.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=HYDE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"Given a question, write a short Wikipedia-style passage that \"\n",
    "                \"would answer it. Write it as a factual paragraph, not as a Q&A. \"\n",
    "                \"It does not need to be correct -- just sound like a real \"\n",
    "                \"Wikipedia article about the topic. Keep it under 150 words. \"\n",
    "                \"Output ONLY the passage, no preamble.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def hyde_retrieve(\n",
    "    question: str,\n",
    "    collection: chromadb.Collection,\n",
    "    top_k: int,\n",
    ") -> tuple[list[str], list[str], str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using HyDE: generate a hypothetical answer,\n",
    "    embed it, and search for similar real documents.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (doc_ids, doc_texts, hypothetical_doc)\n",
    "    \"\"\"\n",
    "    hypo_doc = generate_hypothetical_document(question)\n",
    "\n",
    "    # Embed the hypothetical document (not the question)\n",
    "    hypo_embedding = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL, input=[hypo_doc]\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Search using the hypothetical document's embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[hypo_embedding],\n",
    "        n_results=top_k,\n",
    "    )\n",
    "\n",
    "    ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "    docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "    return ids, docs, hypo_doc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n    \"\"\"\n    Generate an answer using the eval model with retrieved documents.\n\n    Args:\n        question: The question to answer\n        retrieved_docs: List of document texts retrieved from the collection\n\n    Returns:\n        The generated answer as a string\n    \"\"\"\n    context = \"\\n\\n\".join(\n        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n\n    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\nYour task is to:\n1. Carefully read all provided documents\n2. Find the information needed to answer the question\n3. Provide a clear, concise answer based ONLY on the documents\n\nIf the answer cannot be found in the documents, say so explicitly.\"\"\"\n\n    user_prompt = f\"\"\"Documents:\n{context}\n\nQuestion: {question}\n\nPlease answer the question based on the provided documents.\"\"\"\n\n    response = client.chat.completions.create(\n        model=EVAL_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        max_tokens=4096,\n    )\n\n    return response.choices[0].message.content or \"\"\n\n\ndef score_answer(\n    question: str,\n    documents: list[str],\n    generated_answer: str,\n) -> tuple[int, str]:\n    \"\"\"\n    Score the generated answer using the scoring model.\n\n    Args:\n        question: The original question\n        documents: The retrieved documents used to generate the answer\n        generated_answer: The answer generated by the eval model\n\n    Returns:\n        Tuple of (score out of 100, explanation)\n    \"\"\"\n    context = \"\\n\\n\".join(\n        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n\n    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\nEvaluate the answer on these criteria:\n1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n\nRespond with a JSON object containing:\n{\n    \"score\": <integer from 0-100>,\n    \"reasoning\": \"<brief explanation of the score>\"\n}\"\"\"\n\n    eval_prompt = f\"\"\"Documents:\n{context}\n\nQuestion: {question}\n\nGenerated Answer:\n{generated_answer}\"\"\"\n\n    response = client.chat.completions.create(\n        model=SCORING_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": eval_prompt},\n        ],\n        max_tokens=300,\n    )\n\n    try:\n        score_data = json.loads(response.choices[0].message.content or \"{}\")\n        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n    except json.JSONDecodeError:\n        return 0, \"Error parsing score response\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate using HyDE retrieval: generate hypothetical doc, embed it,\n",
    "    search for real docs, then answer with the original question.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    # HyDE retrieval\n",
    "    retrieved_ids, retrieved_docs, hypo_doc = hyde_retrieve(\n",
    "        question, collection, top_k\n",
    "    )\n",
    "\n",
    "    hits = len(gold_ids & set(retrieved_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    # Answer with the ORIGINAL question, not the hypothetical doc\n",
    "    generated_answer = generate_answer(question, retrieved_docs)\n",
    "    score, reasoning = score_answer(question, retrieved_docs, generated_answer)\n",
    "\n",
    "    return example_index, {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"hypothetical_doc\": hypo_doc,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries, collection, qrels, top_k, max_workers=8\n",
    ") -> dict:\n",
    "    eval_size = len(eval_queries)\n",
    "    print(f\"Running evaluation on {eval_size} queries with {max_workers} parallel workers...\\n\")\n",
    "    results_by_index, scores, recalls = {}, [], []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(evaluate_single_example, eval_queries[i], i, collection, qrels, top_k): i\n",
    "            for i in range(eval_size)\n",
    "        }\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                completed += 1\n",
    "                print(f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question: {result['question'][:80]}...\")\n",
    "                print(f\"  HyDE doc: {result['hypothetical_doc'][:80]}...\")\n",
    "                print(f\"  Recall@{top_k}: {result['recall_at_k']:.2f} ({result['gold_docs_found']}/{result['gold_docs_total']} gold)\")\n",
    "                print(f\"  Score: {result['score']}/100\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    ordered = [results_by_index[i] for i in range(eval_size) if i in results_by_index]\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL, \"scoring_model\": SCORING_MODEL,\n",
    "        \"hyde_model\": HYDE_MODEL, \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(), \"top_k\": top_k,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(sum(scores)/len(scores), 2) if scores else 0,\n",
    "        \"avg_recall_at_k\": round(sum(recalls)/len(recalls), 4) if recalls else 0,\n",
    "        \"individual_scores\": scores, \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": ordered,\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, TOP_K)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"HyDE model   :    {results['hyde_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.7_hyde.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}