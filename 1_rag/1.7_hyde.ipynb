{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Large Corpus RAG with HyDE\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** flips the retrieval problem: instead of embedding the question, we ask an LLM to generate a hypothetical answer, then embed _that_ and search for real documents similar to it.\n",
    "\n",
    "Why does this work? A question like \"what is the capital of France?\" and a document containing \"Paris is the capital of France\" live in different parts of embedding space -- one is a question, the other is a statement. But a hypothetical answer (\"The capital of France is Paris\") is much closer to the real document.\n",
    "\n",
    "The hypothetical answer doesn't need to be correct. It just needs to _sound like_ the kind of document that would contain the real answer. Even a wrong guess pulls the embedding toward the right neighborhood.\n",
    "\n",
    "Same corpus, same collection, same scoring as 1.4. The only change is what gets embedded for the search query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saadkhalid/Projects/agents/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "HYDE_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "TOP_K = 5\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "# Direct OpenAI client for embedding the hypothetical document\n",
    "openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BEIR/NQ queries and relevance judgments...\n",
      "Selected 50 queries, 61 gold documents\n",
      "Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\n",
      "Locating gold documents in corpus...\n",
      "Found 61/61 gold documents\n",
      "Corpus: 10000 docs (61 gold + 9939 distractors)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit! Reusing collection 'nq_10000_b1bf36b34ec3' (10,000 docs, no re-embedding needed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDE: Hypothetical Document Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_document(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a hypothetical document that would answer the question.\n",
    "\n",
    "    The output doesn't need to be factually correct -- it just needs to\n",
    "    sound like a Wikipedia passage about the topic, so its embedding\n",
    "    lands near real relevant documents.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=HYDE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"Given a question, write a short Wikipedia-style passage that \"\n",
    "                \"would answer it. Write it as a factual paragraph, not as a Q&A. \"\n",
    "                \"It does not need to be correct -- just sound like a real \"\n",
    "                \"Wikipedia article about the topic. Keep it under 150 words. \"\n",
    "                \"Output ONLY the passage, no preamble.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def hyde_retrieve(\n",
    "    question: str,\n",
    "    collection: chromadb.Collection,\n",
    "    top_k: int,\n",
    ") -> tuple[list[str], list[str], str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using HyDE: generate a hypothetical answer,\n",
    "    embed it, and search for similar real documents.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (doc_ids, doc_texts, hypothetical_doc)\n",
    "    \"\"\"\n",
    "    hypo_doc = generate_hypothetical_document(question)\n",
    "\n",
    "    # Embed the hypothetical document (not the question)\n",
    "    hypo_embedding = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL, input=[hypo_doc]\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Search using the hypothetical document's embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[hypo_embedding],\n",
    "        n_results=top_k,\n",
    "    )\n",
    "\n",
    "    ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "    docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "    return ids, docs, hypo_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the eval model with retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        retrieved_docs: List of document texts retrieved from the collection\n",
    "\n",
    "    Returns:\n",
    "        The generated answer as a string\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\n",
    "Your task is to:\n",
    "1. Carefully read all provided documents\n",
    "2. Find the information needed to answer the question\n",
    "3. Provide a clear, concise answer based ONLY on the documents\n",
    "\n",
    "If the answer cannot be found in the documents, say so explicitly.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question based on the provided documents.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=EVAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "def score_answer(\n",
    "    question: str,\n",
    "    documents: list[str],\n",
    "    generated_answer: str,\n",
    ") -> tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Score the generated answer using the scoring model.\n",
    "\n",
    "    Args:\n",
    "        question: The original question\n",
    "        documents: The retrieved documents used to generate the answer\n",
    "        generated_answer: The answer generated by the eval model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (score out of 100, explanation)\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\n",
    "Evaluate the answer on these criteria:\n",
    "1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n",
    "2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n",
    "3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n",
    "4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "{\n",
    "    \"score\": <integer from 0-100>,\n",
    "    \"reasoning\": \"<brief explanation of the score>\"\n",
    "}\"\"\"\n",
    "\n",
    "    eval_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=SCORING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score_data = json.loads(response.choices[0].message.content or \"{}\")\n",
    "        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return 0, \"Error parsing score response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate using HyDE retrieval: generate hypothetical doc, embed it,\n",
    "    search for real docs, then answer with the original question.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    # HyDE retrieval\n",
    "    retrieved_ids, retrieved_docs, hypo_doc = hyde_retrieve(\n",
    "        question, collection, top_k\n",
    "    )\n",
    "\n",
    "    hits = len(gold_ids & set(retrieved_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    # Answer with the ORIGINAL question, not the hypothetical doc\n",
    "    generated_answer = generate_answer(question, retrieved_docs)\n",
    "    score, reasoning = score_answer(question, retrieved_docs, generated_answer)\n",
    "\n",
    "    return example_index, {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"hypothetical_doc\": hypo_doc,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries, collection, qrels, top_k, max_workers=8\n",
    ") -> dict:\n",
    "    eval_size = len(eval_queries)\n",
    "    print(\n",
    "        f\"Running evaluation on {eval_size} queries with {max_workers} parallel workers...\\n\")\n",
    "    results_by_index, scores, recalls = {}, [], []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(evaluate_single_example, eval_queries[i], i, collection, qrels, top_k): i\n",
    "            for i in range(eval_size)\n",
    "        }\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                completed += 1\n",
    "                print(f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question: {result['question'][:80]}...\")\n",
    "                print(f\"  HyDE doc: {result['hypothetical_doc'][:80]}...\")\n",
    "                print(\n",
    "                    f\"  Recall@{top_k}: {result['recall_at_k']:.2f} ({result['gold_docs_found']}/{result['gold_docs_total']} gold)\")\n",
    "                print(f\"  Score: {result['score']}/100\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    ordered = [results_by_index[i]\n",
    "               for i in range(eval_size) if i in results_by_index]\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL, \"scoring_model\": SCORING_MODEL,\n",
    "        \"hyde_model\": HYDE_MODEL, \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(), \"top_k\": top_k,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(sum(scores)/len(scores), 2) if scores else 0,\n",
    "        \"avg_recall_at_k\": round(sum(recalls)/len(recalls), 4) if recalls else 0,\n",
    "        \"individual_scores\": scores, \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": ordered,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 50 queries with 8 parallel workers...\n",
      "\n",
      "[1/50] Example 3\n",
      "  Question: who sings love will keep us alive by the eagles...\n",
      "  HyDE doc: \"Love Will Keep Us Alive\" is a ballad performed by the American rock band Eagles...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[2/50] Example 2\n",
      "  Question: how many episodes are in chicago fire season 4...\n",
      "  HyDE doc: The fourth season of the American drama television series *Chicago Fire* consist...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[3/50] Example 5\n",
      "  Question: nitty gritty dirt band fishin in the dark album...\n",
      "  HyDE doc: \"Fishin' in the Dark\" is a prominent country-rock single by the Nitty Gritty Dir...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[4/50] Example 4\n",
      "  Question: who is the leader of the ontario pc party...\n",
      "  HyDE doc: The current leader of the Progressive Conservative Party of Ontario (Ontario PC ...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[5/50] Example 1\n",
      "  Question: what is non controlling interest on balance sheet...\n",
      "  HyDE doc: In consolidated financial accounting, **non-controlling interest** (NCI), also r...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[6/50] Example 6\n",
      "  Question: who were the three elves who got rings...\n",
      "  HyDE doc: In the fictional legendarium of J.R.R. Tolkien, the Three Rings of the Elves, kn...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[7/50] Example 9\n",
      "  Question: where do characters live in this is us...\n",
      "  HyDE doc: The central characters of the television drama *This Is Us* are primarily situat...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[8/50] Example 10\n",
      "  Question: who makes the decisions about what to produce in a market economy...\n",
      "  HyDE doc: In a market economy, the primary decisions regarding the production of goods and...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[9/50] Example 13\n",
      "  Question: who plays the doc in back to the future...\n",
      "  HyDE doc: In the *Back to the Future* film trilogy, the character of Dr. Emmett \"Doc\" Brow...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[10/50] Example 17\n",
      "  Question: what was the name of atom bomb dropped by usa on hiroshima...\n",
      "  HyDE doc: **Little Boy** was the codename for the type of atomic bomb dropped on the Japan...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[11/50] Example 11\n",
      "  Question: when is the new tappan zee bridge going to be finished...\n",
      "  HyDE doc: The Governor Mario M. Cuomo Bridge, colloquially known as the New Tappan Zee Bri...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[12/50] Example 8\n",
      "  Question: in order to prove disparate impact you first must establish...\n",
      "  HyDE doc: In the legal framework of employment discrimination law, establishing a claim of...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[13/50] Example 12\n",
      "  Question: who recorded i can't help falling in love with you...\n",
      "  HyDE doc: \"Can't Help Falling in Love\" is a classic pop ballad originally recorded by Amer...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[14/50] Example 16\n",
      "  Question: how many seasons of prison break are on netflix...\n",
      "  HyDE doc: As of 2024, the distribution of *Prison Break* on Netflix varies significantly b...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[15/50] Example 21\n",
      "  Question: how many lines of symmetry are there in a equilateral triangle...\n",
      "  HyDE doc: In geometry, an equilateral triangle is a regular polygon with three equal sides...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[16/50] Example 18\n",
      "  Question: when did the us take over wake island...\n",
      "  HyDE doc: Wake Island was formally claimed by the United States on January 17, 1899, durin...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[17/50] Example 15\n",
      "  Question: who has been chosen as the brand ambassador of the campaign 'beti bachao-beti pa...\n",
      "  HyDE doc: The **Beti Bachao, Beti Padhao** (Save the Girl Child, Educate the Girl Child) c...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[18/50] Example 20\n",
      "  Question: where are the mitochondria located in the sperm...\n",
      "  HyDE doc: In the mammalian spermatozoon, the mitochondria are localized exclusively within...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[19/50] Example 22\n",
      "  Question: how many seasons of the oc are there...\n",
      "  HyDE doc: *The OC* is an American teen drama television series created by Josh Schwartz th...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[20/50] Example 26\n",
      "  Question: what is pumped up kicks the song about...\n",
      "  HyDE doc: \"Pumped Up Kicks\" is the debut single by the American indie pop band Foster the ...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[21/50] Example 25\n",
      "  Question: who were farmers who kept a small portion of their crops & gave the rest to the ...\n",
      "  HyDE doc: In agricultural history, sharecroppers were tenant farmers who worked land belon...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[22/50] Example 14\n",
      "  Question: when did they stop cigarette advertising on television...\n",
      "  HyDE doc: The prohibition of cigarette advertising on broadcast media was a landmark shift...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[23/50] Example 27\n",
      "  Question: 2. what are the reasons states impose protectionists policies on other countries...\n",
      "  HyDE doc: Protectionist policies, such as tariffs, import quotas, and government subsidies...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[24/50] Example 7\n",
      "  Question: converting stereo signal to mono signal is called...\n",
      "  HyDE doc: In audio engineering, the process of combining a two-channel stereo signal into ...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 85/100\n",
      "\n",
      "[25/50] Example 23\n",
      "  Question: latest season on keeping up with the kardashians...\n",
      "  HyDE doc: The twentieth and final season of the American reality television series *Keepin...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[26/50] Example 29\n",
      "  Question: what event provoked congress to propose the eleventh amendment and the states to...\n",
      "  HyDE doc: The Eleventh Amendment to the United States Constitution was proposed and ratifi...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[27/50] Example 34\n",
      "  Question: is spain the second largest country in europe...\n",
      "  HyDE doc: In terms of total land area, Spain is the second-largest country in the European...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[28/50] Example 24\n",
      "  Question: how dose the poet present death as a voyage in crossing the bar...\n",
      "  HyDE doc: In Alfred, Lord Tennyson's 1889 poem \"Crossing the Bar,\" death is extendedly met...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[29/50] Example 31\n",
      "  Question: what is the meaning of the name comanche...\n",
      "  HyDE doc: The ethonym \"Comanche\" is derived from the Ute word *kÉ¨mantsi*, which translates...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[30/50] Example 28\n",
      "  Question: where are they building the new raiders stadium...\n",
      "  HyDE doc: Allegiant Stadium, the purpose-built home of the NFL's Las Vegas Raiders, is sit...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[31/50] Example 30\n",
      "  Question: does joe die in the purge election year...\n",
      "  HyDE doc: In the 2016 film *The Purge: Election Year*, the character Joe Dixon, portrayed ...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[32/50] Example 35\n",
      "  Question: difference between single layer perceptron and multilayer perceptron...\n",
      "  HyDE doc: The primary distinction between a single-layer perceptron (SLP) and a multilayer...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[33/50] Example 33\n",
      "  Question: what are bulls used for on a farm...\n",
      "  HyDE doc: In agricultural management, bulls serve as the primary biological component for ...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[34/50] Example 40\n",
      "  Question: royal society for the protection of birds number of members...\n",
      "  HyDE doc: The Royal Society for the Protection of Birds (RSPB) is the largest wildlife con...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[35/50] Example 41\n",
      "  Question: when was theme from a summer place released...\n",
      "  HyDE doc: \"Theme from A Summer Place\" was first released in 1959 as part of the soundtrack...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[36/50] Example 32\n",
      "  Question: who do you meet at the gates of heaven...\n",
      "  HyDE doc: In religious iconography and folk tradition, the entrance to the celestial realm...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "\n",
      "[37/50] Example 37\n",
      "  Question: who is edmund on days of our lives...\n",
      "  HyDE doc: In the NBC daytime soap opera *Days of Our Lives*, Edmund Crumb is a character i...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[38/50] Example 43\n",
      "  Question: when did athens emerges as wealthiest greek city state...\n",
      "  HyDE doc: Athens emerged as the preeminent economic power of the Greek world during the mi...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[39/50] Example 42\n",
      "  Question: when does jenny humphrey come back to gossip girl...\n",
      "  HyDE doc: Jenny Humphrey, portrayed by Taylor Momsen, makes her final appearance as a seri...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[40/50] Example 36\n",
      "  Question: what is the meaning of the dragon boat festival...\n",
      "  HyDE doc: The Dragon Boat Festival, known in Mandarin as the Duanwu Festival, is a traditi...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[41/50] Example 45\n",
      "  Question: who sings war don't let me down...\n",
      "  HyDE doc: \"War (Don't Let Me Down)\" is a soul-inflected rock single released in 1974 by th...\n",
      "  Recall@5: 0.00 (0/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[42/50] Example 19\n",
      "  Question: when did american two party system began to emerge...\n",
      "  HyDE doc: The American two-party system began to emerge during the mid-1790s, rooted in di...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[43/50] Example 47\n",
      "  Question: who plays v on orange is the new black...\n",
      "  HyDE doc: In the Netflix comedy-drama series *Orange Is the New Black*, the character Yvon...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[44/50] Example 39\n",
      "  Question: what degree is a crock pot on low...\n",
      "  HyDE doc: In a standard slow cooker, the \"Low\" setting typically reaches an internal tempe...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[45/50] Example 46\n",
      "  Question: who invented the frisbee and how did it get its name...\n",
      "  HyDE doc: The modern plastic flying disc was invented by Walter Frederick Morrison in 1948...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[46/50] Example 48\n",
      "  Question: how long prime minister stay in office canada...\n",
      "  HyDE doc: In Canada, the Prime Minister does not serve a fixed term of office and may rema...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[47/50] Example 38\n",
      "  Question: i want to be with you everywhere song...\n",
      "  HyDE doc: \"Everywhere\" is a song by the British-American rock band Fleetwood Mac, released...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[48/50] Example 50\n",
      "  Question: when do willow and tara get back together...\n",
      "  HyDE doc: In the sixth season of the television series *Buffy the Vampire Slayer*, the cha...\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "\n",
      "[49/50] Example 49\n",
      "  Question: where was the movie jeremiah johnson filmed at...\n",
      "  HyDE doc: Principal photography for the 1972 film *Jeremiah Johnson* took place almost ent...\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"HyDE model   :    {results['hyde_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.7_hyde.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
