{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Large Corpus RAG with Query Rewriting\n",
    "\n",
    "Same setup as 1.4 -- BEIR/NQ dataset, shared 10K+ doc ChromaDB collection, Recall@K -- but with one change: before searching, we rewrite the user's question into a search-optimized query using an LLM.\n",
    "\n",
    "User questions are conversational (\"who sang that song in the movie?\"). Embedding search works better with keyword-rich, declarative statements (\"1993 film Aerosmith song soundtrack director\"). The rewrite model transforms one into the other.\n",
    "\n",
    "We search with the **rewritten** query but answer with the **original** question. The rewrite is optimized for retrieval, not comprehension. Everything else (corpus, collection, scoring) is identical to 1.4 so the comparison is controlled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Any, cast\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenRouter client with the OpenAI SDK\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "# Model being evaluated\n",
    "EVAL_MODEL = \"moonshotai/kimi-k2.5\"\n",
    "\n",
    "# Scoring model\n",
    "SCORING_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "# Query rewriting model\n",
    "REWRITE_MODEL = \"google/gemini-3-flash-preview\"\n",
    "\n",
    "# RAG retrieval settings\n",
    "TOP_K = 5\n",
    "CORPUS_SIZE = 10000\n",
    "NUM_EXAMPLES = 50\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \".chroma_nq\"\n",
    "\n",
    "# Embedding function for ChromaDB (uses OpenAI directly, not OpenRouter)\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=EMBEDDING_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BEIR/NQ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(corpus_size: int, num_examples: int) -> tuple[list[dict], list[dict], dict]:\n",
    "    \"\"\"\n",
    "    Load BEIR Natural Questions: corpus, queries, and relevance judgments.\n",
    "\n",
    "    Builds a corpus subset that includes all gold-relevant documents for\n",
    "    the queries we'll evaluate, plus randomly sampled Wikipedia passages\n",
    "    as distractors up to corpus_size.\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Total number of documents in the corpus subset\n",
    "        num_examples: Number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (corpus_subset, eval_queries, qrels)\n",
    "        - corpus_subset: list of dicts with _id, title, text\n",
    "        - eval_queries: list of dicts with _id, text\n",
    "        - qrels: dict mapping query_id -> set of relevant doc_ids\n",
    "    \"\"\"\n",
    "    print(\"Loading BEIR/NQ queries and relevance judgments...\")\n",
    "    # The BeIR/nq dataset uses a custom loading script that datasets v4+\n",
    "    # no longer supports. Load directly from the auto-converted parquet files.\n",
    "    queries_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"hf://datasets/BeIR/nq@refs/convert/parquet/queries/queries/0000.parquet\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    qrels_ds = load_dataset(\"BeIR/nq-qrels\", split=\"test\")\n",
    "\n",
    "    # Build qrels lookup: query_id -> set of relevant corpus doc_ids\n",
    "    qrels: dict[str, set[str]] = {}\n",
    "    for row in qrels_ds:\n",
    "        qrels.setdefault(row[\"query-id\"], set()).add(row[\"corpus-id\"])\n",
    "\n",
    "    # Select queries that have relevance judgments\n",
    "    eval_queries = [q for q in queries_ds if q[\"_id\"] in qrels][:num_examples]\n",
    "\n",
    "    # Collect all gold-relevant doc IDs for the queries we'll evaluate\n",
    "    gold_doc_ids: set[str] = set()\n",
    "    for q in eval_queries:\n",
    "        gold_doc_ids.update(qrels[q[\"_id\"]])\n",
    "\n",
    "    print(\n",
    "        f\"Selected {len(eval_queries)} queries, {len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Load the full corpus (HuggingFace uses memory-mapped Arrow, so this\n",
    "    # doesn't load 2.68M docs into RAM -- it's a lazy view)\n",
    "    print(\"Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\")\n",
    "    corpus_ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=[\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0000.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0001.parquet\",\n",
    "            \"hf://datasets/BeIR/nq@refs/convert/parquet/corpus/corpus/0002.parquet\",\n",
    "        ],\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Find gold documents using HF's optimized batched filter\n",
    "    print(\"Locating gold documents in corpus...\")\n",
    "    gold_docs_ds = corpus_ds.filter(\n",
    "        lambda batch: [did in gold_doc_ids for did in batch[\"_id\"]],\n",
    "        batched=True,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    print(f\"Found {len(gold_docs_ds)}/{len(gold_doc_ids)} gold documents\")\n",
    "\n",
    "    # Build corpus subset: gold docs + random distractors\n",
    "    # Skip docs with empty text (OpenAI embeddings API rejects them)\n",
    "    corpus_subset = [\n",
    "        {\"_id\": gold_docs_ds[i][\"_id\"],\n",
    "         \"text\": gold_docs_ds[i][\"text\"],\n",
    "         \"title\": gold_docs_ds[i][\"title\"]}\n",
    "        for i in range(len(gold_docs_ds))\n",
    "        if gold_docs_ds[i][\"text\"]\n",
    "    ]\n",
    "\n",
    "    # Sample random distractor documents from the corpus\n",
    "    fill_count = max(0, corpus_size - len(corpus_subset))\n",
    "    if fill_count > 0:\n",
    "        random.seed(42)\n",
    "        # Sample candidate indices, then filter out any gold docs\n",
    "        candidate_indices = random.sample(\n",
    "            range(len(corpus_ds)), min(fill_count * 2, len(corpus_ds)))\n",
    "        candidates = corpus_ds.select(candidate_indices)\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            if candidates[i][\"_id\"] not in gold_doc_ids and candidates[i][\"text\"]:\n",
    "                corpus_subset.append({\n",
    "                    \"_id\": candidates[i][\"_id\"],\n",
    "                    \"text\": candidates[i][\"text\"],\n",
    "                    \"title\": candidates[i][\"title\"],\n",
    "                })\n",
    "            if len(corpus_subset) >= corpus_size:\n",
    "                break\n",
    "\n",
    "    gold_count = len(gold_docs_ds)\n",
    "    fill_actual = len(corpus_subset) - gold_count\n",
    "    print(f\"Corpus: {len(corpus_subset)} docs \"\n",
    "          f\"({gold_count} gold + {fill_actual} distractors)\\n\")\n",
    "\n",
    "    return corpus_subset, eval_queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BEIR/NQ queries and relevance judgments...\n",
      "Selected 50 queries, 61 gold documents\n",
      "Loading BEIR/NQ corpus (2.68M Wikipedia passages)...\n",
      "Locating gold documents in corpus...\n",
      "Found 61/61 gold documents\n",
      "Corpus: 10000 docs (61 gold + 9939 distractors)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_subset, eval_queries, qrels = load_data(CORPUS_SIZE, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ChromaDB Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collection(\n",
    "    corpus_subset: list[dict],\n",
    "    chroma_dir: str,\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Build or load a persistent ChromaDB collection from the corpus subset.\n",
    "\n",
    "    Uses a stable collection name based on a hash of the document IDs, so\n",
    "    re-running with the same corpus skips embedding entirely (even across\n",
    "    sessions). If the corpus changes, a new collection is created.\n",
    "\n",
    "    Args:\n",
    "        corpus_subset: List of document dicts with _id, text, title\n",
    "        chroma_dir: Path to ChromaDB persistent storage directory\n",
    "\n",
    "    Returns:\n",
    "        A ChromaDB Collection ready for querying\n",
    "    \"\"\"\n",
    "    # Build a stable name from the sorted doc IDs so the same corpus\n",
    "    # always maps to the same collection (and we skip re-embedding)\n",
    "    id_hash = hashlib.sha256(\n",
    "        \",\".join(sorted(d[\"_id\"] for d in corpus_subset)).encode()\n",
    "    ).hexdigest()[:12]\n",
    "    collection_name = f\"nq_{len(corpus_subset)}_{id_hash}\"\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_dir)\n",
    "\n",
    "    # Try to reuse an existing collection with matching name and size\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=cast(Any, embedding_fn),\n",
    "        )\n",
    "        if collection.count() == len(corpus_subset):\n",
    "            print(f\"Cache hit! Reusing collection '{collection_name}' \"\n",
    "                  f\"({collection.count():,} docs, no re-embedding needed)\\n\")\n",
    "            return collection\n",
    "        # Size mismatch -- rebuild\n",
    "        print(f\"Collection size mismatch \"\n",
    "              f\"({collection.count()} vs {len(corpus_subset)}), rebuilding...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    total = len(corpus_subset)\n",
    "    print(\n",
    "        f\"Embedding {total:,} documents into collection '{collection_name}'...\")\n",
    "    print(f\"(This is a one-time cost -- cached on disk for future runs)\\n\")\n",
    "\n",
    "    # Embed and add documents in batches with timing stats\n",
    "    BATCH_SIZE = 500\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=cast(Any, embedding_fn),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_num, i in enumerate(range(0, total, BATCH_SIZE), 1):\n",
    "        batch = corpus_subset[i:i + BATCH_SIZE]\n",
    "        batch_start = time.time()\n",
    "\n",
    "        collection.add(\n",
    "            ids=[d[\"_id\"] for d in batch],\n",
    "            documents=[d[\"text\"] for d in batch],\n",
    "            metadatas=[{\"title\": d[\"title\"]} for d in batch],\n",
    "        )\n",
    "\n",
    "        done = min(i + BATCH_SIZE, total)\n",
    "        batch_time = time.time() - batch_start\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = done / elapsed  # docs per second\n",
    "        remaining = (total - done) / rate if rate > 0 else 0\n",
    "\n",
    "        print(f\"  [{batch_num}/{total_batches}] {done:,}/{total:,} docs | \"\n",
    "              f\"batch: {batch_time:.1f}s | \"\n",
    "              f\"rate: {rate:.0f} docs/s | \"\n",
    "              f\"ETA: {remaining:.0f}s\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCollection ready: {collection.count():,} documents \"\n",
    "          f\"(embedded in {total_time:.1f}s)\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit! Reusing collection 'nq_10000_b1bf36b34ec3' (10,000 docs, no re-embedding needed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection = build_collection(corpus_subset, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Rewriting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite a user question into a search-optimized query.\n",
    "\n",
    "    Conversational questions don't always match well against document\n",
    "    embeddings. This function asks an LLM to rewrite the question into\n",
    "    a keyword-rich, declarative form that's better suited for embedding\n",
    "    similarity search.\n",
    "\n",
    "    Args:\n",
    "        question: The original user question\n",
    "\n",
    "    Returns:\n",
    "        A search-optimized query string\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=REWRITE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are a search query optimizer. Given a user question, \"\n",
    "                \"rewrite it as a search query optimized for semantic similarity \"\n",
    "                \"search against a Wikipedia corpus. Follow these rules:\\n\"\n",
    "                \"1. Extract key entities, names, dates, and concepts\\n\"\n",
    "                \"2. Use declarative, keyword-rich phrasing instead of question form\\n\"\n",
    "                \"3. Expand abbreviations and add synonyms where helpful\\n\"\n",
    "                \"4. Remove filler words (who, what, how, please, etc.)\\n\"\n",
    "                \"5. Keep it concise -- one line, no explanation\\n\"\n",
    "                \"6. Output ONLY the rewritten query, nothing else\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Score Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, retrieved_docs: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the eval model with retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        retrieved_docs: List of document texts retrieved from the collection\n",
    "\n",
    "    Returns:\n",
    "        The generated answer as a string\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on provided documents.\n",
    "Your task is to:\n",
    "1. Carefully read all provided documents\n",
    "2. Find the information needed to answer the question\n",
    "3. Provide a clear, concise answer based ONLY on the documents\n",
    "\n",
    "If the answer cannot be found in the documents, say so explicitly.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question based on the provided documents.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=EVAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "def score_answer(\n",
    "    question: str,\n",
    "    documents: list[str],\n",
    "    generated_answer: str,\n",
    ") -> tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Score the generated answer using the scoring model.\n",
    "\n",
    "    Args:\n",
    "        question: The original question\n",
    "        documents: The retrieved documents used to generate the answer\n",
    "        generated_answer: The answer generated by the eval model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (score out of 100, explanation)\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"[Document {i+1}]\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of answers to questions.\n",
    "Evaluate the answer on these criteria:\n",
    "1. Correctness (0-25 points): Is the answer factually accurate based on the documents?\n",
    "2. Completeness (0-25 points): Does it fully answer the question? Are important details included?\n",
    "3. Faithfulness (0-25 points): Does it only use information from the documents? No hallucinations?\n",
    "4. Clarity (0-25 points): Is the answer clear, well-organized, and easy to understand?\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "{\n",
    "    \"score\": <integer from 0-100>,\n",
    "    \"reasoning\": \"<brief explanation of the score>\"\n",
    "}\"\"\"\n",
    "\n",
    "    eval_prompt = f\"\"\"Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=SCORING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score_data = json.loads(response.choices[0].message.content or \"{}\")\n",
    "        return score_data.get(\"score\", 0), score_data.get(\"reasoning\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return 0, \"Error parsing score response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_example(\n",
    "    query: dict,\n",
    "    example_index: int,\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate a single query against the shared collection.\n",
    "\n",
    "    Rewrites the query for better retrieval, retrieves top-k documents,\n",
    "    computes Recall@K against ground-truth relevance labels, generates\n",
    "    an answer using the ORIGINAL question, and scores it.\n",
    "    \"\"\"\n",
    "    question = query[\"text\"]\n",
    "    query_id = query[\"_id\"]\n",
    "    gold_ids = qrels.get(query_id, set())\n",
    "\n",
    "    # Rewrite the question into a search-optimized query\n",
    "    rewritten = rewrite_query(question)\n",
    "\n",
    "    # Retrieve using the REWRITTEN query (not the original question)\n",
    "    results = collection.query(query_texts=[rewritten], n_results=top_k)\n",
    "\n",
    "    retrieved_ids = results[\"ids\"][0] if results[\"ids\"] else []\n",
    "    retrieved_docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "    # Compute Recall@K: what fraction of gold docs did we find?\n",
    "    hits = len(gold_ids & set(retrieved_ids))\n",
    "    recall = hits / len(gold_ids) if gold_ids else 0.0\n",
    "\n",
    "    # Generate answer using the ORIGINAL question (not rewritten)\n",
    "    generated_answer = generate_answer(question, retrieved_docs)\n",
    "    score, reasoning = score_answer(question, retrieved_docs, generated_answer)\n",
    "\n",
    "    result = {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"rewritten_query\": rewritten,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score,\n",
    "        \"scoring_reasoning\": reasoning,\n",
    "        \"recall_at_k\": recall,\n",
    "        \"gold_docs_found\": hits,\n",
    "        \"gold_docs_total\": len(gold_ids),\n",
    "    }\n",
    "\n",
    "    return example_index, result\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_queries: list[dict],\n",
    "    collection: chromadb.Collection,\n",
    "    qrels: dict[str, set[str]],\n",
    "    top_k: int,\n",
    "    max_workers: int = 8,\n",
    ") -> dict:\n",
    "    \"\"\"Run evaluation on all queries using parallel workers.\"\"\"\n",
    "    eval_size = len(eval_queries)\n",
    "\n",
    "    print(f\"Running evaluation on {eval_size} queries with \"\n",
    "          f\"{max_workers} parallel workers...\\n\")\n",
    "\n",
    "    results_by_index: dict[int, dict] = {}\n",
    "    scores: list[int] = []\n",
    "    recalls: list[float] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(\n",
    "                evaluate_single_example,\n",
    "                eval_queries[i], i, collection, qrels, top_k\n",
    "            ): i\n",
    "            for i in range(eval_size)\n",
    "        }\n",
    "\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                example_idx, result = future.result()\n",
    "                results_by_index[example_idx] = result\n",
    "                scores.append(result[\"score\"])\n",
    "                recalls.append(result[\"recall_at_k\"])\n",
    "                completed += 1\n",
    "\n",
    "                # Print progress\n",
    "                print(\n",
    "                    f\"[{completed}/{eval_size}] Example {example_idx + 1}\")\n",
    "                print(f\"  Question:  {result['question'][:80]}...\")\n",
    "                print(f\"  Rewritten: {result['rewritten_query'][:80]}\")\n",
    "                print(f\"  Recall@{top_k}: {result['recall_at_k']:.2f} \"\n",
    "                      f\"({result['gold_docs_found']}/{result['gold_docs_total']} gold)\")\n",
    "                print(f\"  Score: {result['score']}/100\")\n",
    "                print(f\"  Reasoning: {result['scoring_reasoning']}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Example {idx + 1} failed: {e}\\n\")\n",
    "\n",
    "    results = [results_by_index[i]\n",
    "               for i in range(eval_size) if i in results_by_index]\n",
    "\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "\n",
    "    return {\n",
    "        \"model_evaluated\": EVAL_MODEL,\n",
    "        \"scoring_model\": SCORING_MODEL,\n",
    "        \"rewrite_model\": REWRITE_MODEL,\n",
    "        \"dataset\": \"BeIR/NQ\",\n",
    "        \"corpus_size\": collection.count(),\n",
    "        \"top_k\": top_k,\n",
    "        \"num_examples_evaluated\": len(scores),\n",
    "        \"overall_score\": round(avg_score, 2),\n",
    "        \"avg_recall_at_k\": round(avg_recall, 4),\n",
    "        \"individual_scores\": scores,\n",
    "        \"individual_recalls\": recalls,\n",
    "        \"score_distribution\": {\n",
    "            \"90-100\": sum(1 for s in scores if s >= 90),\n",
    "            \"80-89\": sum(1 for s in scores if 80 <= s < 90),\n",
    "            \"70-79\": sum(1 for s in scores if 70 <= s < 80),\n",
    "            \"60-69\": sum(1 for s in scores if 60 <= s < 70),\n",
    "            \"below-60\": sum(1 for s in scores if s < 60),\n",
    "        },\n",
    "        \"detailed_results\": results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 50 queries with 8 parallel workers...\n",
      "\n",
      "[1/50] Example 3\n",
      "  Question:  who sings love will keep us alive by the eagles...\n",
      "  Rewritten: Timothy B. Schmit lead vocals Love Will Keep Us Alive Eagles Hell Freezes Over b\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate, complete, and faithful to the provided documents. Document 1 explicitly states that the song features lead vocals by bassist Timothy B. Schmit. The answer is also clear and concise.\n",
      "\n",
      "[2/50] Example 4\n",
      "  Question:  who is the leader of the ontario pc party...\n",
      "  Rewritten: Doug Ford leader Ontario Progressive Conservative Party PC Party of Ontario MPP \n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate based on the provided documents. Documents 1 and 2 explicitly state that Patrick Brown is the leader of the Progressive Conservative Party of Ontario. The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[3/50] Example 2\n",
      "  Question:  how many episodes are in chicago fire season 4...\n",
      "  Rewritten: Chicago Fire TV series Season 4 total episode count 2015-2016 NBC drama episodes\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1, which explicitly states that the fourth season contained 23 episodes. The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[4/50] Example 8\n",
      "  Question:  in order to prove disparate impact you first must establish...\n",
      "  Rewritten: disparate impact legal doctrine prima facie case requirements title vii civil ri\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct and faithful to the provided documents. It directly quotes the specific requirement for proving disparate impact as defined in Document 1 and provides a clear explanation based solely on the text.\n",
      "\n",
      "[5/50] Example 5\n",
      "  Question:  nitty gritty dirt band fishin in the dark album...\n",
      "  Rewritten: Nitty Gritty Dirt Band Hold On album Fishin' in the Dark single 1987 country roc\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the documents provided. It correctly identifies the album 'Hold On', the release date, chart performance, and certification status based on Document 1.\n",
      "\n",
      "[6/50] Example 6\n",
      "  Question:  who were the three elves who got rings...\n",
      "  Rewritten: Three Rings of Power Elves Galadriel Elrond Cirdan Gil-galad Nenya Vilya Narya J\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and fully covers the information provided in Document 1. It correctly identifies the three individuals (Gil-galad, Círdan, and Galadriel), the specific rings they received (Vilya, Narya, and Nenya), and acknowledges the different accounts of how they received them as described in the text.\n",
      "\n",
      "[7/50] Example 1\n",
      "  Question:  what is non controlling interest on balance sheet...\n",
      "  Rewritten: non-controlling interest NCI minority interest consolidated balance sheet equity\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and fully addresses the question using only the provided documents. It correctly identifies non-controlling interest as minority interest (Document 2) and explains its presentation on the balance sheet as reflecting the claim on assets belonging to non-controlling shareholders (Document 1).\n",
      "\n",
      "[8/50] Example 13\n",
      "  Question:  who plays the doc in back to the future...\n",
      "  Rewritten: Christopher Lloyd actor character Doctor Emmett Brown Doc Back to the Future fil\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfect. It correctly identifies Christopher Lloyd as the actor, provides his full name from the documents, and cites the specific documents that support the claim. It is clear, concise, and faithful to the provided text.\n",
      "\n",
      "[9/50] Example 12\n",
      "  Question:  who recorded i can't help falling in love with you...\n",
      "  Rewritten: \"Can't Help Falling in Love\" song recording artist Elvis Presley RCA 1961 Blue H\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct, complete, and faithful to the provided documents. It correctly identifies the original artist (Elvis Presley) and the various cover artists mentioned in Document 1. The answer is also clear and well-organized.\n",
      "\n",
      "[10/50] Example 11\n",
      "  Question:  when is the new tappan zee bridge going to be finished...\n",
      "  Rewritten: Governor Mario M. Cuomo Bridge Tappan Zee Bridge replacement completion date con\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate based on Document 1, which explicitly states that both spans are expected to be operational by June 15, 2018. The response is clear, faithful to the text, and directly answers the question.\n",
      "\n",
      "[11/50] Example 9\n",
      "  Question:  where do characters live in this is us...\n",
      "  Rewritten: This Is Us television series filming locations Pearson family residences Pittsbu\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to Document 1, which contains all the relevant information regarding the characters' locations in 'This Is Us'. It correctly identifies where the adult characters live and where they were raised.\n",
      "\n",
      "[12/50] Example 17\n",
      "  Question:  what was the name of atom bomb dropped by usa on hiroshima...\n",
      "  Rewritten: Little Boy atomic bomb United States Hiroshima Japan August 6 1945 Manhattan Pro\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on the provided documents. Document 1 explicitly states that a B-29 dropped a 'Little Boy' uranium gun-type bomb on Hiroshima. The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[13/50] Example 14\n",
      "  Question:  when did they stop cigarette advertising on television...\n",
      "  Rewritten: Public Health Cigarette Smoking Act television radio broadcast ban dates January\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate, faithful to the documents, and provides the specific date mentioned in Document 1. It also correctly identifies the legislation that enacted the ban.\n",
      "\n",
      "[14/50] Example 7\n",
      "  Question:  converting stereo signal to mono signal is called...\n",
      "  Rewritten: audio signal processing stereo-to-mono conversion downmixing summation mono comp\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: \n",
      "\n",
      "[15/50] Example 15\n",
      "  Question:  who has been chosen as the brand ambassador of the campaign 'beti bachao-beti pa...\n",
      "  Rewritten: Beti Bachao Beti Padhao campaign brand ambassadors national state level Sakshi M\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and directly supported by Document 1. It correctly identifies Sakshi Malik as the brand ambassador and provides the specific date mentioned in the text. The answer is clear, faithful to the source, and answers the question fully.\n",
      "\n",
      "[16/50] Example 10\n",
      "  Question:  who makes the decisions about what to produce in a market economy...\n",
      "  Rewritten: market economy production decisions private individuals businesses consumers sup\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate and faithful to the provided documents. It correctly identifies the interplay of supply and demand as the basis for decisions (Document 1) and specifies that investment/allocation decisions are made through capital and financial markets (Document 1). It also provides context by contrasting this with a planned economy as described in the text. The response is clear and concise.\n",
      "\n",
      "[17/50] Example 21\n",
      "  Question:  how many lines of symmetry are there in a equilateral triangle...\n",
      "  Rewritten: equilateral triangle lines of symmetry geometry properties rotation reflectional\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on Document 1, which states that an equilateral triangle has '3 lines of reflection'. The answer is clear, concise, and relies solely on the provided text.\n",
      "\n",
      "[18/50] Example 20\n",
      "  Question:  where are the mitochondria located in the sperm...\n",
      "  Rewritten: mitochondria location sperm cell midpiece mitochondrial helix axoneme structure\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. It correctly identifies the location (midpiece) and specific arrangement (spiralled around the central filamentous core) of the mitochondria by synthesizing information from Document 1 and Document 2. The response is clear and directly addresses the question.\n",
      "\n",
      "[19/50] Example 16\n",
      "  Question:  how many seasons of prison break are on netflix...\n",
      "  Rewritten: Prison Break television series seasons count Netflix streaming availability epis\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and faithful to the provided documents. Document 1 confirms that Prison Break is available on Netflix but does not specify the number of seasons hosted there, only mentioning that five seasons exist on DVD/Blu-ray. The answer correctly identifies this lack of specific information.\n",
      "\n",
      "[20/50] Example 24\n",
      "  Question:  how dose the poet present death as a voyage in crossing the bar...\n",
      "  Rewritten: Alfred Lord Tennyson poem Crossing the Bar nautical metaphor death transition vo\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies the core metaphors from Document 1, explaining how the 'crossing' represents the transition from life to death. It correctly identifies the Pilot as God and explains the nature of the voyage (serene and secure) and the destination (meeting face to face). It adheres strictly to the provided document and is very clearly organized.\n",
      "\n",
      "[21/50] Example 26\n",
      "  Question:  what is pumped up kicks the song about...\n",
      "  Rewritten: Foster the People Pumped Up Kicks song meaning lyrics themes school shooting you\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfect. it accurately reflects the content of Documents 1 and 2, which are the only documents relevant to the song 'Pumped Up Kicks'. It correctly identifies the perspective of the narrator, the themes of mental illness and gun violence, and the meaning of the title. The answer is clear, concise, and remains faithful to the provided text.\n",
      "\n",
      "[22/50] Example 18\n",
      "  Question:  when did the us take over wake island...\n",
      "  Rewritten: United States annexation Wake Island date history 1899 Spanish-American War terr\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[23/50] Example 22\n",
      "  Question:  how many seasons of the oc are there...\n",
      "  Rewritten: The OC television series Fox total number of seasons episodes list\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is correct, complete, and faithful to the provided documents. Document 1 explicitly mentions that the fourth season was the final season of The O.C., confirming there are four seasons in total. The answer is also clear and easy to understand.\n",
      "\n",
      "[24/50] Example 23\n",
      "  Question:  latest season on keeping up with the kardashians...\n",
      "  Rewritten: Keeping Up with the Kardashians final season 20 release date series finale 2021 \n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate and faithful to the provided documents. It correctly identifies from Document 1 that as of December 10, 2017, fourteen seasons had concluded. The answer is clear and directly addresses the question using only the provided information.\n",
      "\n",
      "[25/50] Example 30\n",
      "  Question:  does joe die in the purge election year...\n",
      "  Rewritten: Joe Dixon fate survival death status The Purge: Election Year film character end\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate according to Document 1. It correctly identifies that Joe is fatally wounded and dies after asking Marcos to care for his store. It is clear, concise, and uses only the provided information.\n",
      "\n",
      "[26/50] Example 31\n",
      "  Question:  what is the meaning of the name comanche...\n",
      "  Rewritten: Comanche people ethnonym etymology Numunu Ute language Kohmahts enemy anyone who\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate based on Document 1. It provides the specific Ute word and its meaning as requested by the question. It is clear, concise, and stays strictly within the provided texts.\n",
      "\n",
      "[27/50] Example 19\n",
      "  Question:  when did american two party system began to emerge...\n",
      "  Rewritten: origins United States two-party system Federalist Party Democratic-Republican Pa\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and fully supported by the provided documents. It correctly identifies the emergence of the system in the 1790s (First Party System) using specific evidence from Documents 1 and 4 regarding the Federalist and Democratic-Republican parties. The explanation is clear and remains faithful to the text provided.\n",
      "\n",
      "[28/50] Example 27\n",
      "  Question:  2. what are the reasons states impose protectionists policies on other countries...\n",
      "  Rewritten: economic rationales state protectionism trade barriers tariffs import quotas nat\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and directly derived from the provided documents. It correctly identifies the primary reason for protectionism mentioned in Document 1 (shielding domestic producers/workers) and adds the historical context regarding manufacturing interests from Document 3. It is clear, concise, and faithful to the text.\n",
      "\n",
      "[29/50] Example 28\n",
      "  Question:  where are they building the new raiders stadium...\n",
      "  Rewritten: Allegiant Stadium location Las Vegas Raiders NFL construction site Paradise Neva\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. It correctly identifies the location (Paradise, Nevada) and provides the specific geographical details (62 acres west of Mandalay Bay, specific cross-streets, and proximity to I-15) found in Document 1.\n",
      "\n",
      "[30/50] Example 34\n",
      "  Question:  is spain the second largest country in europe...\n",
      "  Rewritten: Spain land area size comparison European countries ranking geography list\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: \n",
      "\n",
      "[31/50] Example 37\n",
      "  Question:  who is edmund on days of our lives...\n",
      "  Rewritten: Edmund Crumb Days of our Lives NBC soap opera character Adam Caine roles 1997 19\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[32/50] Example 38\n",
      "  Question:  i want to be with you everywhere song...\n",
      "  Rewritten: \"Everywhere\" Fleetwood Mac song Christine McVie Rumours 1987 singles discography\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate and faithful to the provided documents. It correctly identifies the song 'Everywhere' by Fleetwood Mac (which contains the lyrics 'I want to be with you everywhere') and provides all the relevant details mentioned in Document 1, including the artist, album, writer, and chart performance. The information is clearly organized.\n",
      "\n",
      "[33/50] Example 33\n",
      "  Question:  what are bulls used for on a farm...\n",
      "  Rewritten: cattle Bos taurus bull farm uses roles breeding livestock reproduction meat prod\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. it accurately identifies all uses of bulls mentioned in the text (breeding, semen production for AI, and meat production). It correctly distinguishes between bulls and oxen (castrated males) as per Document 2. The answer is faithful to the provided text, clear, and complete.\n",
      "\n",
      "[34/50] Example 39\n",
      "  Question:  what degree is a crock pot on low...\n",
      "  Rewritten: slow cooker crock-pot low temperature setting degrees Fahrenheit Celsius heat ra\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and faithful to the documents. Document 1 provides a temperature range for the 'warming' setting (71–74 °C), but none of the documents specify the temperature for the 'low' setting. The answer correctly identifies this distinction and avoids making up information.\n",
      "\n",
      "[35/50] Example 25\n",
      "  Question:  who were farmers who kept a small portion of their crops & gave the rest to the ...\n",
      "  Rewritten: sharecropping sharecroppers tenant farming agricultural labor systems crop-lien \n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer correctly identifies 'sharecroppers' based on Document 1. It accurately describes the arrangement where a tenant uses land in exchange for a share of the crops produced. The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[36/50] Example 41\n",
      "  Question:  when was theme from a summer place released...\n",
      "  Rewritten: Theme from A Summer Place Max Steiner Percy Faith release date 1959 1960 soundtr\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[37/50] Example 40\n",
      "  Question:  royal society for the protection of birds number of members...\n",
      "  Rewritten: Royal Society for the Protection of Birds RSPB total membership count statistics\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is completely accurate, faithful to the provided documents, and directly addresses the question. It correctly identifies the membership count from Document 1 and includes the relevant detail about youth members.\n",
      "\n",
      "[38/50] Example 32\n",
      "  Question:  who do you meet at the gates of heaven...\n",
      "  Rewritten: Saint Peter gatekeeper Heaven afterlife Christian mythology Keys of the Kingdom \n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: \n",
      "\n",
      "[39/50] Example 42\n",
      "  Question:  when does jenny humphrey come back to gossip girl...\n",
      "  Rewritten: Jenny Humphrey Taylor Momsen return episodes Gossip Girl Season 4 Easy J Damian \n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly accurate and directly addresses the question using Document 1. It correctly identifies the timing of her return (the series finale during a five-year time jump) and provides the context (Dan's wedding). It does not include any outside information.\n",
      "\n",
      "[40/50] Example 29\n",
      "  Question:  what event provoked congress to propose the eleventh amendment and the states to...\n",
      "  Rewritten: Chisholm v. Georgia 1793 Supreme Court case state sovereign immunity Eleventh Am\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and directly supported by Documents 1 and 2. It correctly identifies the Supreme Court case Chisholm v. Georgia as the provocative event and explains the legal context of the ruling as described in the provided text. The response is clear, faithful to the source material, and complete.\n",
      "\n",
      "[41/50] Example 45\n",
      "  Question:  who sings war don't let me down...\n",
      "  Rewritten: War \"Don't Let Me Down\" song artist vocalists group members Eric Burdon lowrider\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and faithful to the provided documents. It correctly identifies that there is no information about an artist named 'War' performing the song, but provides the relevant information about the song title 'Don't Let Me Down' as found in Document 2.\n",
      "\n",
      "[42/50] Example 47\n",
      "  Question:  who plays v on orange is the new black...\n",
      "  Rewritten: Lorraine Toussaint character Yvonne \"Vee\" Parker Orange Is the New Black cast ac\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[43/50] Example 36\n",
      "  Question:  what is the meaning of the dragon boat festival...\n",
      "  Rewritten: Duanwu Festival origins Dragon Boat Festival history meaning Qu Yuan commemorati\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[44/50] Example 50\n",
      "  Question:  when do willow and tara get back together...\n",
      "  Rewritten: Willow Rosenberg Tara Maclay relationship reconciliation episode Buffy the Vampi\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate based on Document 1. It correctly identifies 'Seeing Red' as the episode where the reconciliation is completed, following the courtship started in 'Entropy'. The answer is clear, faithful to the text, and complete.\n",
      "\n",
      "[45/50] Example 43\n",
      "  Question:  when did athens emerges as wealthiest greek city state...\n",
      "  Rewritten: Athens emergence prosperity wealth silver mines Laurium Delian League Fifth Cent\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is perfectly correct according to Document 1. It accurately identifies the time period (late 6th century BCE) and provides the supporting details mentioned in the text regarding silver and trade. It is clear, faithful to the text, and complete.\n",
      "\n",
      "[46/50] Example 46\n",
      "  Question:  who invented the frisbee and how did it get its name...\n",
      "  Rewritten: Fred Morrison Walter Frederick Morrison origin Frisbee name Frisbie Pie Company \n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is excellent. It correctly identifies Fred Morrison as the inventor (as well as his collaborators mentioned in Document 1) and provides a detailed and accurate explanation of how the 'Frisbee' name was adopted, based solely on the provided text. The clarity and organization are also high.\n",
      "\n",
      "[47/50] Example 35\n",
      "  Question:  difference between single layer perceptron and multilayer perceptron...\n",
      "  Rewritten: single layer perceptron vs multilayer perceptron comparison architecture activat\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely accurate based on the provided documents. It correctly identifies the structural, functional, and capability differences between a 'true' (single) perceptron and a multilayer perceptron (MLP) as described in Document 1. It remains faithful to the text without introducing outside information and is very clearly organized.\n",
      "\n",
      "[48/50] Example 48\n",
      "  Question:  how long prime minister stay in office canada...\n",
      "  Rewritten: Canada Prime Minister term length tenure duration limits constitutional conventi\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 0/100\n",
      "  Reasoning: Error parsing score response\n",
      "\n",
      "[49/50] Example 49\n",
      "  Question:  where was the movie jeremiah johnson filmed at...\n",
      "  Rewritten: Jeremiah Johnson film locations Utah Zion National Park Uinta National Forest Sn\n",
      "  Recall@5: 1.00 (2/2 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is accurate and fully supported by the provided documents. It correctly identifies the filming locations in Utah, explains the background of why it was filmed there instead of a studio backlot, and maintains high clarity and faithfulness to the source text.\n",
      "\n",
      "[50/50] Example 44\n",
      "  Question:  why does king from tekken wear a mask...\n",
      "  Rewritten: King Tekken character jaguar mask lore priest orphan luchador Fray Juan Tormenta\n",
      "  Recall@5: 1.00 (1/1 gold)\n",
      "  Score: 100/100\n",
      "  Reasoning: The answer is entirely correct, faithful, and clear. It accurately identifies that the provided documents do not explicitly state the original reason for the mask, but it correctly extracts all relevant details regarding the jaguar mask mentioned in Document 1, including when he gave it up and why he put it back on.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_evaluation(eval_queries, collection, qrels, TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Model evaluated:  moonshotai/kimi-k2.5\n",
      "Scoring model:    google/gemini-3-flash-preview\n",
      "Rewrite model:    google/gemini-3-flash-preview\n",
      "Dataset:          BeIR/NQ\n",
      "Corpus size:      10,000 documents\n",
      "Top-K:            5\n",
      "Queries evaluated: 50\n",
      "\n",
      "Avg Recall@5:    0.9800\n",
      "Overall Score:    96.0/100\n",
      "\n",
      "Score Distribution:\n",
      "  90-100: 48 examples\n",
      "  80-89: 0 examples\n",
      "  70-79: 0 examples\n",
      "  60-69: 0 examples\n",
      "  below-60: 2 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model evaluated:  {results['model_evaluated']}\")\n",
    "print(f\"Scoring model:    {results['scoring_model']}\")\n",
    "print(f\"Rewrite model:    {results['rewrite_model']}\")\n",
    "print(f\"Dataset:          {results['dataset']}\")\n",
    "print(f\"Corpus size:      {results['corpus_size']:,} documents\")\n",
    "print(f\"Top-K:            {results['top_k']}\")\n",
    "print(f\"Queries evaluated: {results['num_examples_evaluated']}\")\n",
    "print(f\"\\nAvg Recall@{TOP_K}:    {results['avg_recall_at_k']:.4f}\")\n",
    "print(f\"Overall Score:    {results['overall_score']}/100\")\n",
    "print(f\"\\nScore Distribution:\")\n",
    "for range_label, count in results['score_distribution'].items():\n",
    "    print(f\"  {range_label}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to evals/1.5_large_corpus_rag_with_query_rewriting.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to ./evals\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "eval_path = \"evals/1.5_large_corpus_rag_with_query_rewriting.json\"\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {eval_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
